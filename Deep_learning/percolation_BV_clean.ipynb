{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "german-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "experimental-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # this includes tensor functions that we can use in backwards pass\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-conditioning",
   "metadata": {},
   "source": [
    "# Constructing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "magnetic-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does a depth-first search across a matrix of 0s and 1s, \n",
    "# looking for a path of 1s between the first and last columns of the matrix\n",
    "# such a path is called a 'percolation path'\n",
    "\n",
    "def percolate( mat ):\n",
    "    \"\"\"\n",
    "    Returns True if there is a percolation path of 1s from col 0 col -1 of a matrix of 0s and 1s\n",
    "    \"\"\"\n",
    "    nrows = mat.shape[0]\n",
    "    ncols = mat.shape[1]\n",
    "    frontier = set()\n",
    "    for i in range(0,nrows):\n",
    "        if mat[i,0]:\n",
    "            frontier.add( (i,0) )\n",
    "    explored = set()\n",
    "    flag = False # this will be returned if the frontier becomes empty without finding a\n",
    "                 # filled pixel in the right-most column\n",
    "    while frontier: # frontier evaluates to True in this context if it is non-empty\n",
    "        r,c = frontier.pop()\n",
    "        explored.add( (r,c) )\n",
    "        if r > 0: # North\n",
    "            if mat[r-1,c]:\n",
    "                coords = (r-1,c)\n",
    "                if coords not in explored: \n",
    "                    if coords not in frontier: # this order of testing is necessary since each element of explored has been in frontier\n",
    "                        frontier.add( coords )\n",
    "        if c < ncols-1: # East\n",
    "            if mat[r,c+1]:\n",
    "                if c+1 == mat.shape[1]-1 : # Hurray, we have percolated to the last column\n",
    "                    flag = True\n",
    "                    break\n",
    "                coords = (r,c+1)\n",
    "                if coords not in explored: \n",
    "                    if coords not in frontier: # this order of testing is necessary since each element of explored has been in frontier\n",
    "                        frontier.add( coords )\n",
    "        if r < nrows-1: # South\n",
    "            if mat[r+1,c]:\n",
    "                coords = (r+1,c)\n",
    "                if coords not in explored: \n",
    "                    if coords not in frontier: # this order of testing is necessary since each element of explored has been in frontier\n",
    "                        frontier.add( coords )\n",
    "        if c > 0: # West\n",
    "            if mat[r,c-1]:\n",
    "                coords = (r,c-1)\n",
    "                if coords not in explored: \n",
    "                    if coords not in frontier: # this order of testing is necessary since each element of explored has been in frontier\n",
    "                        frontier.add( coords )\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_percolation_dataset(side=8,threshold=0.42,n_examples=10):\n",
    "    \"\"\"\n",
    "    This function generates an array of random images, in the form needed for pytorch, and \n",
    "    then labels them as percolati# ng or not, using the percolate function. Roughly 50% of the\n",
    "    images will have class 1 (percolating), so the dataset is likely to be reasonably balanced. \n",
    "    \"\"\"\n",
    "    X_data = (np.random.random([n_examples,side,side,1]) > threshold).astype(float) \n",
    "    Y_data = np.zeros([n_examples,1])\n",
    "    for i in range(0,n_examples):\n",
    "        if percolate(X_data[i,:,:,0]):\n",
    "            Y_data[i,0] = 1\n",
    "    return [(torch.tensor(x.astype(np.float32)).reshape(1,8,8), torch.tensor(float(y)).reshape(1) )  for x,y in zip( X_data, Y_data)]\n",
    "\n",
    "# the types of the numbers in the tensors need to be float32: the most natural place to do this data conversion\n",
    "# is in generating the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "excited-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = make_percolation_dataset( n_examples = 1000)\n",
    "test_set = make_percolation_dataset( n_examples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "about-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.493"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this calculates the fraction of positive examples in the training set\n",
    "sum( [ x[1].item() for x in training_set ] ) / len( training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "vital-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7fe5fb21e190>, tensor([0.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check that a random element of the training set is correctly labelled\n",
    "x,y = training_set[401]\n",
    "plt.imshow(x.numpy().squeeze() ), y  # check a random element of the training set to see it is correct\n",
    "# we need to squeeze the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-attack",
   "metadata": {},
   "source": [
    "# Define neural net training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "digital-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a neural-net class as argument, \n",
    "# together with the number of epochs, datasets, and batch-size\n",
    "# I have written it like this to make sure you remember to initialise a new NN \n",
    "# (However you can redefine it to take a pre-initialised NN, so that you can repeatedly \n",
    "# train a NN with the same weights, as often as you want. )\n",
    "\n",
    "def define_and_train( NN_class, n_epochs, training_set, test_set, batch_size=32, weight_decay=0.0 ):\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader( training_set, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader( test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    thenet = NN_class()\n",
    "    optimizer1 = optim.Adam( thenet.parameters(), weight_decay=weight_decay )\n",
    "    \n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for epoch in range(n_epochs): # number of times to loop over the dataset\n",
    "        \n",
    "        total_loss = 0 \n",
    "        total_correct = 0 \n",
    "        total_examples = 0 \n",
    "        n_mini_batches = 0\n",
    "    \n",
    "        for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "            images, labels = mini_batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            # all the parameters that are being updated are in the optimizer, \n",
    "            # so if we zero the gradients of all the tensors in the optimizer, \n",
    "            # that is the safest way to zero all the gradients\n",
    "            optimizer1.zero_grad()\n",
    "\n",
    "            outputs = thenet(images) # this is the forward pass\n",
    "\n",
    "            loss = loss_function ( outputs, labels )\n",
    "\n",
    "            loss.backward() # does the backward pass and computes all gradients\n",
    "\n",
    "            optimizer1.step() # does one optimisation step\n",
    "\n",
    "            n_mini_batches += 1 # keep track of number of minibatches, and collect the loss for each minibatch\n",
    "            total_loss += loss.item() # remember that the loss is a zero-order tensor\n",
    "            # so that to extract its value, we use .item(), as we cannot index as there are no dimensions\n",
    "\n",
    "            # keep track of number of examples, and collect number correct in each minibatch\n",
    "            total_correct += sum( ( outputs > 0.5 ) == ( labels > 0.5 ) ).item()\n",
    "            total_examples += len( labels )\n",
    "\n",
    "        # calculate statistics for each epoch and print them. \n",
    "        # You can alter this code to accumulate these statistics into lists/vectors and plot them\n",
    "        epoch_training_accuracy = total_correct / total_examples\n",
    "        epoch_training_loss = total_loss / n_mini_batches\n",
    "\n",
    "        epoch_val_accuracy, epoch_val_loss = accuracy_and_loss( thenet, loss_function, testloader )\n",
    "\n",
    "        print('Epoch %d loss: %.3f acc: %.3f val_loss: %.3f val_acc: %.3f'\n",
    "              %(epoch+1, epoch_training_loss, epoch_training_accuracy, epoch_val_loss, epoch_val_accuracy   ))\n",
    "        \n",
    "        train_loss.append( epoch_training_loss )\n",
    "        train_acc.append( epoch_training_accuracy )\n",
    "        val_loss.append( epoch_val_loss )\n",
    "        val_acc.append( epoch_val_accuracy )\n",
    "    \n",
    "    history = { 'train_loss': train_loss, \n",
    "                'train_acc': train_acc, \n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc }\n",
    "    return ( history, thenet ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-concern",
   "metadata": {},
   "source": [
    "# Defining accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expired-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the loss and error rate on the validation set\n",
    "# assuming that the output of the neural net nn is a single prediction probability, from a single sigmoid neuron. \n",
    "\n",
    "def accuracy_and_loss( net, loss_function, dataloader ):\n",
    "    total_correct = 0 \n",
    "    total_loss = 0.0 \n",
    "    total_examples = 0 \n",
    "    n_batches = 0 \n",
    "    with torch.no_grad():  # we do not neet to compute the gradients when making predictions on the validation set\n",
    "        for data in dataloader: \n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            batch_loss = loss_function(outputs, labels) # this is averaged over the batch\n",
    "            n_batches += 1\n",
    "            total_loss += batch_loss.item()\n",
    "            total_correct += sum( (outputs > 0.5 ) == ( labels > 0.5 ) ).item() # number correct in the minibatch\n",
    "            total_examples += labels.size(0) # the number of labels, which is just the size of the minibatch \n",
    "             \n",
    "    \n",
    "    accuracy = total_correct / total_examples\n",
    "    mean_loss = total_loss / n_batches\n",
    "    \n",
    "    return ( accuracy, mean_loss )\n",
    "    #print( \"Accuracy on test set: %d %%\" %(100 * correct/total_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-iceland",
   "metadata": {},
   "source": [
    "# Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cubic-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-bernard",
   "metadata": {},
   "source": [
    "# Option 1: Basic convolutional net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "silent-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional1( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional1, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "arabic-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.693 acc: 0.494 val_loss: 0.693 val_acc: 0.535\n",
      "Epoch 2 loss: 0.693 acc: 0.512 val_loss: 0.692 val_acc: 0.540\n",
      "Epoch 3 loss: 0.687 acc: 0.527 val_loss: 0.687 val_acc: 0.484\n",
      "Epoch 4 loss: 0.678 acc: 0.549 val_loss: 0.673 val_acc: 0.665\n",
      "Epoch 5 loss: 0.667 acc: 0.618 val_loss: 0.656 val_acc: 0.663\n",
      "Epoch 6 loss: 0.636 acc: 0.672 val_loss: 0.653 val_acc: 0.611\n",
      "Epoch 7 loss: 0.628 acc: 0.678 val_loss: 0.626 val_acc: 0.679\n",
      "Epoch 8 loss: 0.621 acc: 0.709 val_loss: 0.617 val_acc: 0.697\n",
      "Epoch 9 loss: 0.599 acc: 0.713 val_loss: 0.602 val_acc: 0.701\n",
      "Epoch 10 loss: 0.602 acc: 0.708 val_loss: 0.590 val_acc: 0.708\n",
      "Epoch 11 loss: 0.581 acc: 0.719 val_loss: 0.597 val_acc: 0.704\n",
      "Epoch 12 loss: 0.572 acc: 0.729 val_loss: 0.586 val_acc: 0.691\n",
      "Epoch 13 loss: 0.559 acc: 0.746 val_loss: 0.573 val_acc: 0.736\n",
      "Epoch 14 loss: 0.560 acc: 0.725 val_loss: 0.553 val_acc: 0.732\n",
      "Epoch 15 loss: 0.521 acc: 0.776 val_loss: 0.540 val_acc: 0.746\n",
      "Epoch 16 loss: 0.519 acc: 0.763 val_loss: 0.534 val_acc: 0.753\n",
      "Epoch 17 loss: 0.496 acc: 0.772 val_loss: 0.530 val_acc: 0.742\n",
      "Epoch 18 loss: 0.493 acc: 0.778 val_loss: 0.534 val_acc: 0.734\n",
      "Epoch 19 loss: 0.477 acc: 0.789 val_loss: 0.537 val_acc: 0.759\n",
      "Epoch 20 loss: 0.457 acc: 0.793 val_loss: 0.485 val_acc: 0.775\n",
      "Epoch 21 loss: 0.467 acc: 0.795 val_loss: 0.514 val_acc: 0.760\n",
      "Epoch 22 loss: 0.437 acc: 0.819 val_loss: 0.540 val_acc: 0.756\n",
      "Epoch 23 loss: 0.441 acc: 0.808 val_loss: 0.453 val_acc: 0.787\n",
      "Epoch 24 loss: 0.414 acc: 0.834 val_loss: 0.442 val_acc: 0.805\n",
      "Epoch 25 loss: 0.386 acc: 0.844 val_loss: 0.431 val_acc: 0.806\n",
      "Epoch 26 loss: 0.376 acc: 0.833 val_loss: 0.418 val_acc: 0.820\n",
      "Epoch 27 loss: 0.365 acc: 0.849 val_loss: 0.456 val_acc: 0.812\n",
      "Epoch 28 loss: 0.350 acc: 0.861 val_loss: 0.422 val_acc: 0.806\n",
      "Epoch 29 loss: 0.354 acc: 0.857 val_loss: 0.412 val_acc: 0.826\n",
      "Epoch 30 loss: 0.352 acc: 0.852 val_loss: 0.389 val_acc: 0.830\n",
      "Epoch 31 loss: 0.347 acc: 0.854 val_loss: 0.374 val_acc: 0.830\n",
      "Epoch 32 loss: 0.347 acc: 0.846 val_loss: 0.368 val_acc: 0.852\n",
      "Epoch 33 loss: 0.313 acc: 0.881 val_loss: 0.369 val_acc: 0.840\n",
      "Epoch 34 loss: 0.324 acc: 0.871 val_loss: 0.375 val_acc: 0.831\n",
      "Epoch 35 loss: 0.324 acc: 0.867 val_loss: 0.364 val_acc: 0.835\n",
      "Epoch 36 loss: 0.325 acc: 0.873 val_loss: 0.367 val_acc: 0.844\n",
      "Epoch 37 loss: 0.313 acc: 0.869 val_loss: 0.341 val_acc: 0.850\n",
      "Epoch 38 loss: 0.296 acc: 0.881 val_loss: 0.389 val_acc: 0.828\n",
      "Epoch 39 loss: 0.296 acc: 0.882 val_loss: 0.362 val_acc: 0.843\n",
      "Epoch 40 loss: 0.281 acc: 0.889 val_loss: 0.372 val_acc: 0.846\n",
      "Epoch 41 loss: 0.278 acc: 0.883 val_loss: 0.346 val_acc: 0.854\n",
      "Epoch 42 loss: 0.306 acc: 0.881 val_loss: 0.342 val_acc: 0.868\n",
      "Epoch 43 loss: 0.283 acc: 0.880 val_loss: 0.347 val_acc: 0.854\n",
      "Epoch 44 loss: 0.279 acc: 0.889 val_loss: 0.362 val_acc: 0.842\n",
      "Epoch 45 loss: 0.280 acc: 0.881 val_loss: 0.334 val_acc: 0.865\n",
      "Epoch 46 loss: 0.266 acc: 0.893 val_loss: 0.346 val_acc: 0.855\n",
      "Epoch 47 loss: 0.249 acc: 0.899 val_loss: 0.329 val_acc: 0.855\n",
      "Epoch 48 loss: 0.256 acc: 0.900 val_loss: 0.319 val_acc: 0.856\n",
      "Epoch 49 loss: 0.258 acc: 0.910 val_loss: 0.330 val_acc: 0.868\n",
      "Epoch 50 loss: 0.247 acc: 0.905 val_loss: 0.323 val_acc: 0.852\n",
      "Epoch 51 loss: 0.250 acc: 0.904 val_loss: 0.349 val_acc: 0.861\n",
      "Epoch 52 loss: 0.258 acc: 0.899 val_loss: 0.326 val_acc: 0.870\n",
      "Epoch 53 loss: 0.235 acc: 0.899 val_loss: 0.322 val_acc: 0.864\n",
      "Epoch 54 loss: 0.225 acc: 0.915 val_loss: 0.321 val_acc: 0.879\n",
      "Epoch 55 loss: 0.221 acc: 0.910 val_loss: 0.309 val_acc: 0.867\n",
      "Epoch 56 loss: 0.232 acc: 0.901 val_loss: 0.299 val_acc: 0.865\n",
      "Epoch 57 loss: 0.243 acc: 0.908 val_loss: 0.316 val_acc: 0.876\n",
      "Epoch 58 loss: 0.243 acc: 0.911 val_loss: 0.301 val_acc: 0.865\n",
      "Epoch 59 loss: 0.230 acc: 0.903 val_loss: 0.358 val_acc: 0.861\n",
      "Epoch 60 loss: 0.211 acc: 0.913 val_loss: 0.359 val_acc: 0.867\n",
      "Epoch 61 loss: 0.235 acc: 0.912 val_loss: 0.327 val_acc: 0.862\n",
      "Epoch 62 loss: 0.222 acc: 0.921 val_loss: 0.321 val_acc: 0.867\n",
      "Epoch 63 loss: 0.221 acc: 0.916 val_loss: 0.318 val_acc: 0.867\n",
      "Epoch 64 loss: 0.202 acc: 0.917 val_loss: 0.298 val_acc: 0.882\n",
      "Epoch 65 loss: 0.209 acc: 0.923 val_loss: 0.295 val_acc: 0.870\n",
      "Epoch 66 loss: 0.217 acc: 0.917 val_loss: 0.305 val_acc: 0.881\n",
      "Epoch 67 loss: 0.208 acc: 0.910 val_loss: 0.374 val_acc: 0.862\n",
      "Epoch 68 loss: 0.216 acc: 0.915 val_loss: 0.298 val_acc: 0.875\n",
      "Epoch 69 loss: 0.218 acc: 0.908 val_loss: 0.298 val_acc: 0.890\n",
      "Epoch 70 loss: 0.204 acc: 0.929 val_loss: 0.302 val_acc: 0.873\n",
      "Epoch 71 loss: 0.222 acc: 0.910 val_loss: 0.332 val_acc: 0.882\n",
      "Epoch 72 loss: 0.220 acc: 0.920 val_loss: 0.295 val_acc: 0.878\n",
      "Epoch 73 loss: 0.207 acc: 0.917 val_loss: 0.287 val_acc: 0.880\n",
      "Epoch 74 loss: 0.210 acc: 0.919 val_loss: 0.308 val_acc: 0.881\n",
      "Epoch 75 loss: 0.195 acc: 0.929 val_loss: 0.298 val_acc: 0.881\n",
      "Epoch 76 loss: 0.201 acc: 0.928 val_loss: 0.303 val_acc: 0.882\n",
      "Epoch 77 loss: 0.191 acc: 0.921 val_loss: 0.302 val_acc: 0.887\n",
      "Epoch 78 loss: 0.205 acc: 0.920 val_loss: 0.308 val_acc: 0.879\n",
      "Epoch 79 loss: 0.194 acc: 0.924 val_loss: 0.293 val_acc: 0.883\n",
      "Epoch 80 loss: 0.206 acc: 0.922 val_loss: 0.290 val_acc: 0.874\n",
      "Epoch 81 loss: 0.220 acc: 0.919 val_loss: 0.294 val_acc: 0.885\n",
      "Epoch 82 loss: 0.188 acc: 0.931 val_loss: 0.301 val_acc: 0.870\n",
      "Epoch 83 loss: 0.207 acc: 0.918 val_loss: 0.296 val_acc: 0.887\n",
      "Epoch 84 loss: 0.183 acc: 0.925 val_loss: 0.284 val_acc: 0.886\n",
      "Epoch 85 loss: 0.189 acc: 0.925 val_loss: 0.278 val_acc: 0.880\n",
      "Epoch 86 loss: 0.199 acc: 0.923 val_loss: 0.351 val_acc: 0.872\n",
      "Epoch 87 loss: 0.181 acc: 0.936 val_loss: 0.282 val_acc: 0.883\n",
      "Epoch 88 loss: 0.172 acc: 0.934 val_loss: 0.312 val_acc: 0.879\n",
      "Epoch 89 loss: 0.169 acc: 0.929 val_loss: 0.350 val_acc: 0.879\n",
      "Epoch 90 loss: 0.200 acc: 0.917 val_loss: 0.271 val_acc: 0.894\n",
      "Epoch 91 loss: 0.179 acc: 0.931 val_loss: 0.303 val_acc: 0.885\n",
      "Epoch 92 loss: 0.192 acc: 0.923 val_loss: 0.274 val_acc: 0.888\n",
      "Epoch 93 loss: 0.193 acc: 0.926 val_loss: 0.283 val_acc: 0.884\n",
      "Epoch 94 loss: 0.159 acc: 0.937 val_loss: 0.293 val_acc: 0.876\n",
      "Epoch 95 loss: 0.211 acc: 0.912 val_loss: 0.293 val_acc: 0.886\n",
      "Epoch 96 loss: 0.148 acc: 0.943 val_loss: 0.317 val_acc: 0.875\n",
      "Epoch 97 loss: 0.186 acc: 0.926 val_loss: 0.301 val_acc: 0.883\n",
      "Epoch 98 loss: 0.193 acc: 0.928 val_loss: 0.282 val_acc: 0.892\n",
      "Epoch 99 loss: 0.166 acc: 0.947 val_loss: 0.285 val_acc: 0.886\n",
      "Epoch 100 loss: 0.191 acc: 0.925 val_loss: 0.293 val_acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "history_1, net_1 = define_and_train(NN_convolutional1, 100, training_set, test_set, batch_size=32, weight_decay=0.0000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-friendship",
   "metadata": {},
   "source": [
    "# Option 2: Convolutional neural net with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "about-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional2( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional2, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caroline-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.696 acc: 0.493 val_loss: 0.695 val_acc: 0.482\n",
      "Epoch 2 loss: 0.694 acc: 0.492 val_loss: 0.693 val_acc: 0.484\n",
      "Epoch 3 loss: 0.691 acc: 0.543 val_loss: 0.688 val_acc: 0.598\n",
      "Epoch 4 loss: 0.684 acc: 0.591 val_loss: 0.676 val_acc: 0.525\n",
      "Epoch 5 loss: 0.658 acc: 0.570 val_loss: 0.643 val_acc: 0.628\n",
      "Epoch 6 loss: 0.629 acc: 0.642 val_loss: 0.606 val_acc: 0.679\n",
      "Epoch 7 loss: 0.579 acc: 0.700 val_loss: 0.555 val_acc: 0.711\n",
      "Epoch 8 loss: 0.553 acc: 0.716 val_loss: 0.619 val_acc: 0.664\n",
      "Epoch 9 loss: 0.534 acc: 0.736 val_loss: 0.552 val_acc: 0.713\n",
      "Epoch 10 loss: 0.512 acc: 0.743 val_loss: 0.563 val_acc: 0.702\n",
      "Epoch 11 loss: 0.543 acc: 0.714 val_loss: 0.537 val_acc: 0.728\n",
      "Epoch 12 loss: 0.502 acc: 0.756 val_loss: 0.529 val_acc: 0.721\n",
      "Epoch 13 loss: 0.504 acc: 0.758 val_loss: 0.541 val_acc: 0.738\n",
      "Epoch 14 loss: 0.504 acc: 0.755 val_loss: 0.560 val_acc: 0.733\n",
      "Epoch 15 loss: 0.503 acc: 0.756 val_loss: 0.517 val_acc: 0.744\n",
      "Epoch 16 loss: 0.492 acc: 0.767 val_loss: 0.507 val_acc: 0.758\n",
      "Epoch 17 loss: 0.503 acc: 0.767 val_loss: 0.507 val_acc: 0.746\n",
      "Epoch 18 loss: 0.481 acc: 0.759 val_loss: 0.517 val_acc: 0.748\n",
      "Epoch 19 loss: 0.478 acc: 0.773 val_loss: 0.494 val_acc: 0.759\n",
      "Epoch 20 loss: 0.465 acc: 0.788 val_loss: 0.536 val_acc: 0.742\n",
      "Epoch 21 loss: 0.473 acc: 0.776 val_loss: 0.514 val_acc: 0.750\n",
      "Epoch 22 loss: 0.478 acc: 0.771 val_loss: 0.498 val_acc: 0.756\n",
      "Epoch 23 loss: 0.463 acc: 0.780 val_loss: 0.501 val_acc: 0.764\n",
      "Epoch 24 loss: 0.453 acc: 0.785 val_loss: 0.501 val_acc: 0.759\n",
      "Epoch 25 loss: 0.454 acc: 0.802 val_loss: 0.474 val_acc: 0.784\n",
      "Epoch 26 loss: 0.427 acc: 0.806 val_loss: 0.495 val_acc: 0.761\n",
      "Epoch 27 loss: 0.440 acc: 0.783 val_loss: 0.459 val_acc: 0.782\n",
      "Epoch 28 loss: 0.427 acc: 0.804 val_loss: 0.453 val_acc: 0.782\n",
      "Epoch 29 loss: 0.407 acc: 0.814 val_loss: 0.459 val_acc: 0.774\n",
      "Epoch 30 loss: 0.408 acc: 0.804 val_loss: 0.480 val_acc: 0.782\n",
      "Epoch 31 loss: 0.412 acc: 0.810 val_loss: 0.429 val_acc: 0.798\n",
      "Epoch 32 loss: 0.392 acc: 0.808 val_loss: 0.434 val_acc: 0.794\n",
      "Epoch 33 loss: 0.390 acc: 0.812 val_loss: 0.428 val_acc: 0.795\n",
      "Epoch 34 loss: 0.399 acc: 0.822 val_loss: 0.447 val_acc: 0.796\n",
      "Epoch 35 loss: 0.382 acc: 0.829 val_loss: 0.436 val_acc: 0.802\n",
      "Epoch 36 loss: 0.372 acc: 0.829 val_loss: 0.423 val_acc: 0.817\n",
      "Epoch 37 loss: 0.369 acc: 0.829 val_loss: 0.441 val_acc: 0.789\n",
      "Epoch 38 loss: 0.382 acc: 0.828 val_loss: 0.418 val_acc: 0.804\n",
      "Epoch 39 loss: 0.355 acc: 0.853 val_loss: 0.439 val_acc: 0.807\n",
      "Epoch 40 loss: 0.373 acc: 0.822 val_loss: 0.404 val_acc: 0.808\n",
      "Epoch 41 loss: 0.328 acc: 0.850 val_loss: 0.424 val_acc: 0.817\n",
      "Epoch 42 loss: 0.342 acc: 0.853 val_loss: 0.398 val_acc: 0.827\n",
      "Epoch 43 loss: 0.357 acc: 0.850 val_loss: 0.376 val_acc: 0.813\n",
      "Epoch 44 loss: 0.337 acc: 0.854 val_loss: 0.372 val_acc: 0.825\n",
      "Epoch 45 loss: 0.313 acc: 0.862 val_loss: 0.396 val_acc: 0.811\n",
      "Epoch 46 loss: 0.333 acc: 0.857 val_loss: 0.389 val_acc: 0.817\n",
      "Epoch 47 loss: 0.348 acc: 0.846 val_loss: 0.381 val_acc: 0.826\n",
      "Epoch 48 loss: 0.324 acc: 0.855 val_loss: 0.361 val_acc: 0.830\n",
      "Epoch 49 loss: 0.317 acc: 0.864 val_loss: 0.380 val_acc: 0.825\n",
      "Epoch 50 loss: 0.303 acc: 0.873 val_loss: 0.363 val_acc: 0.842\n",
      "Epoch 51 loss: 0.307 acc: 0.864 val_loss: 0.342 val_acc: 0.838\n",
      "Epoch 52 loss: 0.285 acc: 0.868 val_loss: 0.358 val_acc: 0.839\n",
      "Epoch 53 loss: 0.299 acc: 0.873 val_loss: 0.366 val_acc: 0.839\n",
      "Epoch 54 loss: 0.276 acc: 0.874 val_loss: 0.333 val_acc: 0.844\n",
      "Epoch 55 loss: 0.286 acc: 0.870 val_loss: 0.320 val_acc: 0.854\n",
      "Epoch 56 loss: 0.288 acc: 0.867 val_loss: 0.339 val_acc: 0.843\n",
      "Epoch 57 loss: 0.295 acc: 0.880 val_loss: 0.340 val_acc: 0.850\n",
      "Epoch 58 loss: 0.272 acc: 0.882 val_loss: 0.330 val_acc: 0.850\n",
      "Epoch 59 loss: 0.266 acc: 0.879 val_loss: 0.325 val_acc: 0.840\n",
      "Epoch 60 loss: 0.259 acc: 0.890 val_loss: 0.329 val_acc: 0.854\n",
      "Epoch 61 loss: 0.240 acc: 0.896 val_loss: 0.346 val_acc: 0.858\n",
      "Epoch 62 loss: 0.283 acc: 0.887 val_loss: 0.340 val_acc: 0.850\n",
      "Epoch 63 loss: 0.273 acc: 0.880 val_loss: 0.354 val_acc: 0.843\n",
      "Epoch 64 loss: 0.264 acc: 0.890 val_loss: 0.326 val_acc: 0.870\n",
      "Epoch 65 loss: 0.231 acc: 0.900 val_loss: 0.305 val_acc: 0.863\n",
      "Epoch 66 loss: 0.254 acc: 0.898 val_loss: 0.311 val_acc: 0.864\n",
      "Epoch 67 loss: 0.231 acc: 0.901 val_loss: 0.319 val_acc: 0.856\n",
      "Epoch 68 loss: 0.230 acc: 0.903 val_loss: 0.340 val_acc: 0.846\n",
      "Epoch 69 loss: 0.223 acc: 0.910 val_loss: 0.337 val_acc: 0.855\n",
      "Epoch 70 loss: 0.246 acc: 0.890 val_loss: 0.326 val_acc: 0.854\n",
      "Epoch 71 loss: 0.218 acc: 0.916 val_loss: 0.314 val_acc: 0.856\n",
      "Epoch 72 loss: 0.242 acc: 0.895 val_loss: 0.309 val_acc: 0.869\n",
      "Epoch 73 loss: 0.225 acc: 0.911 val_loss: 0.326 val_acc: 0.865\n",
      "Epoch 74 loss: 0.210 acc: 0.910 val_loss: 0.298 val_acc: 0.868\n",
      "Epoch 75 loss: 0.206 acc: 0.920 val_loss: 0.309 val_acc: 0.868\n",
      "Epoch 76 loss: 0.218 acc: 0.911 val_loss: 0.304 val_acc: 0.875\n",
      "Epoch 77 loss: 0.202 acc: 0.924 val_loss: 0.317 val_acc: 0.874\n",
      "Epoch 78 loss: 0.191 acc: 0.930 val_loss: 0.294 val_acc: 0.870\n",
      "Epoch 79 loss: 0.218 acc: 0.906 val_loss: 0.314 val_acc: 0.864\n",
      "Epoch 80 loss: 0.212 acc: 0.909 val_loss: 0.300 val_acc: 0.868\n",
      "Epoch 81 loss: 0.206 acc: 0.915 val_loss: 0.281 val_acc: 0.878\n",
      "Epoch 82 loss: 0.195 acc: 0.918 val_loss: 0.303 val_acc: 0.871\n",
      "Epoch 83 loss: 0.218 acc: 0.910 val_loss: 0.290 val_acc: 0.882\n",
      "Epoch 84 loss: 0.197 acc: 0.922 val_loss: 0.301 val_acc: 0.868\n",
      "Epoch 85 loss: 0.182 acc: 0.926 val_loss: 0.296 val_acc: 0.876\n",
      "Epoch 86 loss: 0.212 acc: 0.914 val_loss: 0.284 val_acc: 0.880\n",
      "Epoch 87 loss: 0.182 acc: 0.927 val_loss: 0.283 val_acc: 0.879\n",
      "Epoch 88 loss: 0.186 acc: 0.929 val_loss: 0.327 val_acc: 0.882\n",
      "Epoch 89 loss: 0.188 acc: 0.927 val_loss: 0.283 val_acc: 0.878\n",
      "Epoch 90 loss: 0.181 acc: 0.932 val_loss: 0.297 val_acc: 0.871\n",
      "Epoch 91 loss: 0.188 acc: 0.924 val_loss: 0.302 val_acc: 0.876\n",
      "Epoch 92 loss: 0.181 acc: 0.924 val_loss: 0.295 val_acc: 0.878\n",
      "Epoch 93 loss: 0.180 acc: 0.920 val_loss: 0.299 val_acc: 0.883\n",
      "Epoch 94 loss: 0.171 acc: 0.927 val_loss: 0.264 val_acc: 0.890\n",
      "Epoch 95 loss: 0.173 acc: 0.926 val_loss: 0.296 val_acc: 0.886\n",
      "Epoch 96 loss: 0.190 acc: 0.926 val_loss: 0.252 val_acc: 0.895\n",
      "Epoch 97 loss: 0.170 acc: 0.931 val_loss: 0.270 val_acc: 0.889\n",
      "Epoch 98 loss: 0.170 acc: 0.936 val_loss: 0.274 val_acc: 0.874\n",
      "Epoch 99 loss: 0.171 acc: 0.935 val_loss: 0.263 val_acc: 0.887\n",
      "Epoch 100 loss: 0.190 acc: 0.931 val_loss: 0.279 val_acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "history_2, net_2 = define_and_train(NN_convolutional2, 100, training_set, test_set, batch_size=32, weight_decay=0.0000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-state",
   "metadata": {},
   "source": [
    "Adding dropout makes the performance comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-christopher",
   "metadata": {},
   "source": [
    "# Option 3: Basic Convolutional net with L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tribal-olympus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.694 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 2 loss: 0.693 acc: 0.508 val_loss: 0.691 val_acc: 0.528\n",
      "Epoch 3 loss: 0.692 acc: 0.513 val_loss: 0.690 val_acc: 0.523\n",
      "Epoch 4 loss: 0.689 acc: 0.553 val_loss: 0.692 val_acc: 0.522\n",
      "Epoch 5 loss: 0.687 acc: 0.576 val_loss: 0.691 val_acc: 0.522\n",
      "Epoch 6 loss: 0.680 acc: 0.592 val_loss: 0.675 val_acc: 0.576\n",
      "Epoch 7 loss: 0.655 acc: 0.625 val_loss: 0.638 val_acc: 0.654\n",
      "Epoch 8 loss: 0.617 acc: 0.659 val_loss: 0.601 val_acc: 0.687\n",
      "Epoch 9 loss: 0.570 acc: 0.724 val_loss: 0.566 val_acc: 0.705\n",
      "Epoch 10 loss: 0.558 acc: 0.706 val_loss: 0.557 val_acc: 0.714\n",
      "Epoch 11 loss: 0.538 acc: 0.706 val_loss: 0.548 val_acc: 0.714\n",
      "Epoch 12 loss: 0.518 acc: 0.746 val_loss: 0.554 val_acc: 0.721\n",
      "Epoch 13 loss: 0.517 acc: 0.754 val_loss: 0.565 val_acc: 0.705\n",
      "Epoch 14 loss: 0.494 acc: 0.758 val_loss: 0.544 val_acc: 0.729\n",
      "Epoch 15 loss: 0.506 acc: 0.765 val_loss: 0.542 val_acc: 0.729\n",
      "Epoch 16 loss: 0.489 acc: 0.768 val_loss: 0.508 val_acc: 0.742\n",
      "Epoch 17 loss: 0.481 acc: 0.768 val_loss: 0.544 val_acc: 0.721\n",
      "Epoch 18 loss: 0.472 acc: 0.774 val_loss: 0.521 val_acc: 0.750\n",
      "Epoch 19 loss: 0.460 acc: 0.777 val_loss: 0.508 val_acc: 0.760\n",
      "Epoch 20 loss: 0.466 acc: 0.776 val_loss: 0.556 val_acc: 0.730\n",
      "Epoch 21 loss: 0.471 acc: 0.787 val_loss: 0.491 val_acc: 0.753\n",
      "Epoch 22 loss: 0.472 acc: 0.783 val_loss: 0.497 val_acc: 0.764\n",
      "Epoch 23 loss: 0.453 acc: 0.787 val_loss: 0.491 val_acc: 0.774\n",
      "Epoch 24 loss: 0.471 acc: 0.765 val_loss: 0.481 val_acc: 0.768\n",
      "Epoch 25 loss: 0.454 acc: 0.789 val_loss: 0.485 val_acc: 0.782\n",
      "Epoch 26 loss: 0.435 acc: 0.820 val_loss: 0.483 val_acc: 0.773\n",
      "Epoch 27 loss: 0.434 acc: 0.803 val_loss: 0.471 val_acc: 0.783\n",
      "Epoch 28 loss: 0.434 acc: 0.802 val_loss: 0.499 val_acc: 0.754\n",
      "Epoch 29 loss: 0.437 acc: 0.791 val_loss: 0.467 val_acc: 0.763\n",
      "Epoch 30 loss: 0.425 acc: 0.799 val_loss: 0.480 val_acc: 0.770\n",
      "Epoch 31 loss: 0.410 acc: 0.810 val_loss: 0.459 val_acc: 0.792\n",
      "Epoch 32 loss: 0.413 acc: 0.818 val_loss: 0.463 val_acc: 0.787\n",
      "Epoch 33 loss: 0.407 acc: 0.814 val_loss: 0.463 val_acc: 0.791\n",
      "Epoch 34 loss: 0.404 acc: 0.827 val_loss: 0.460 val_acc: 0.768\n",
      "Epoch 35 loss: 0.400 acc: 0.831 val_loss: 0.458 val_acc: 0.794\n",
      "Epoch 36 loss: 0.387 acc: 0.822 val_loss: 0.475 val_acc: 0.782\n",
      "Epoch 37 loss: 0.383 acc: 0.836 val_loss: 0.436 val_acc: 0.795\n",
      "Epoch 38 loss: 0.391 acc: 0.840 val_loss: 0.425 val_acc: 0.797\n",
      "Epoch 39 loss: 0.372 acc: 0.845 val_loss: 0.424 val_acc: 0.804\n",
      "Epoch 40 loss: 0.380 acc: 0.836 val_loss: 0.419 val_acc: 0.814\n",
      "Epoch 41 loss: 0.364 acc: 0.851 val_loss: 0.411 val_acc: 0.801\n",
      "Epoch 42 loss: 0.366 acc: 0.840 val_loss: 0.441 val_acc: 0.806\n",
      "Epoch 43 loss: 0.348 acc: 0.843 val_loss: 0.408 val_acc: 0.814\n",
      "Epoch 44 loss: 0.341 acc: 0.858 val_loss: 0.401 val_acc: 0.813\n",
      "Epoch 45 loss: 0.343 acc: 0.859 val_loss: 0.412 val_acc: 0.816\n",
      "Epoch 46 loss: 0.338 acc: 0.849 val_loss: 0.395 val_acc: 0.829\n",
      "Epoch 47 loss: 0.314 acc: 0.861 val_loss: 0.380 val_acc: 0.837\n",
      "Epoch 48 loss: 0.321 acc: 0.866 val_loss: 0.395 val_acc: 0.822\n",
      "Epoch 49 loss: 0.304 acc: 0.875 val_loss: 0.407 val_acc: 0.813\n",
      "Epoch 50 loss: 0.357 acc: 0.837 val_loss: 0.386 val_acc: 0.830\n",
      "Epoch 51 loss: 0.313 acc: 0.865 val_loss: 0.401 val_acc: 0.817\n",
      "Epoch 52 loss: 0.308 acc: 0.873 val_loss: 0.375 val_acc: 0.828\n",
      "Epoch 53 loss: 0.307 acc: 0.868 val_loss: 0.363 val_acc: 0.834\n",
      "Epoch 54 loss: 0.312 acc: 0.864 val_loss: 0.407 val_acc: 0.827\n",
      "Epoch 55 loss: 0.329 acc: 0.857 val_loss: 0.362 val_acc: 0.838\n",
      "Epoch 56 loss: 0.286 acc: 0.892 val_loss: 0.379 val_acc: 0.842\n",
      "Epoch 57 loss: 0.305 acc: 0.867 val_loss: 0.362 val_acc: 0.840\n",
      "Epoch 58 loss: 0.293 acc: 0.877 val_loss: 0.344 val_acc: 0.842\n",
      "Epoch 59 loss: 0.284 acc: 0.880 val_loss: 0.348 val_acc: 0.846\n",
      "Epoch 60 loss: 0.277 acc: 0.878 val_loss: 0.367 val_acc: 0.833\n",
      "Epoch 61 loss: 0.278 acc: 0.879 val_loss: 0.360 val_acc: 0.850\n",
      "Epoch 62 loss: 0.261 acc: 0.890 val_loss: 0.378 val_acc: 0.833\n",
      "Epoch 63 loss: 0.277 acc: 0.879 val_loss: 0.366 val_acc: 0.840\n",
      "Epoch 64 loss: 0.278 acc: 0.871 val_loss: 0.382 val_acc: 0.840\n",
      "Epoch 65 loss: 0.274 acc: 0.885 val_loss: 0.353 val_acc: 0.839\n",
      "Epoch 66 loss: 0.282 acc: 0.880 val_loss: 0.354 val_acc: 0.855\n",
      "Epoch 67 loss: 0.272 acc: 0.883 val_loss: 0.358 val_acc: 0.850\n",
      "Epoch 68 loss: 0.260 acc: 0.893 val_loss: 0.351 val_acc: 0.858\n",
      "Epoch 69 loss: 0.252 acc: 0.896 val_loss: 0.361 val_acc: 0.835\n",
      "Epoch 70 loss: 0.288 acc: 0.884 val_loss: 0.353 val_acc: 0.838\n",
      "Epoch 71 loss: 0.257 acc: 0.899 val_loss: 0.326 val_acc: 0.863\n",
      "Epoch 72 loss: 0.251 acc: 0.905 val_loss: 0.343 val_acc: 0.843\n",
      "Epoch 73 loss: 0.261 acc: 0.894 val_loss: 0.340 val_acc: 0.859\n",
      "Epoch 74 loss: 0.251 acc: 0.892 val_loss: 0.353 val_acc: 0.844\n",
      "Epoch 75 loss: 0.260 acc: 0.899 val_loss: 0.356 val_acc: 0.859\n",
      "Epoch 76 loss: 0.243 acc: 0.899 val_loss: 0.334 val_acc: 0.854\n",
      "Epoch 77 loss: 0.226 acc: 0.913 val_loss: 0.336 val_acc: 0.869\n",
      "Epoch 78 loss: 0.243 acc: 0.897 val_loss: 0.303 val_acc: 0.861\n",
      "Epoch 79 loss: 0.245 acc: 0.907 val_loss: 0.349 val_acc: 0.848\n",
      "Epoch 80 loss: 0.236 acc: 0.904 val_loss: 0.343 val_acc: 0.858\n",
      "Epoch 81 loss: 0.224 acc: 0.909 val_loss: 0.362 val_acc: 0.849\n",
      "Epoch 82 loss: 0.254 acc: 0.904 val_loss: 0.329 val_acc: 0.860\n",
      "Epoch 83 loss: 0.234 acc: 0.909 val_loss: 0.327 val_acc: 0.858\n",
      "Epoch 84 loss: 0.230 acc: 0.901 val_loss: 0.319 val_acc: 0.877\n",
      "Epoch 85 loss: 0.230 acc: 0.894 val_loss: 0.342 val_acc: 0.855\n",
      "Epoch 86 loss: 0.216 acc: 0.907 val_loss: 0.343 val_acc: 0.867\n",
      "Epoch 87 loss: 0.222 acc: 0.905 val_loss: 0.321 val_acc: 0.856\n",
      "Epoch 88 loss: 0.199 acc: 0.916 val_loss: 0.320 val_acc: 0.875\n",
      "Epoch 89 loss: 0.213 acc: 0.914 val_loss: 0.352 val_acc: 0.845\n",
      "Epoch 90 loss: 0.186 acc: 0.923 val_loss: 0.359 val_acc: 0.864\n",
      "Epoch 91 loss: 0.238 acc: 0.901 val_loss: 0.339 val_acc: 0.865\n",
      "Epoch 92 loss: 0.231 acc: 0.908 val_loss: 0.365 val_acc: 0.867\n",
      "Epoch 93 loss: 0.197 acc: 0.919 val_loss: 0.304 val_acc: 0.868\n",
      "Epoch 94 loss: 0.213 acc: 0.916 val_loss: 0.327 val_acc: 0.871\n",
      "Epoch 95 loss: 0.197 acc: 0.916 val_loss: 0.340 val_acc: 0.860\n",
      "Epoch 96 loss: 0.219 acc: 0.914 val_loss: 0.321 val_acc: 0.875\n",
      "Epoch 97 loss: 0.186 acc: 0.928 val_loss: 0.325 val_acc: 0.871\n",
      "Epoch 98 loss: 0.188 acc: 0.925 val_loss: 0.372 val_acc: 0.856\n",
      "Epoch 99 loss: 0.188 acc: 0.928 val_loss: 0.342 val_acc: 0.862\n",
      "Epoch 100 loss: 0.202 acc: 0.914 val_loss: 0.298 val_acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "history_3, net_3 = define_and_train(NN_convolutional1, 100, training_set, test_set, batch_size=32, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-volleyball",
   "metadata": {},
   "source": [
    "Comment: L2 regularisation does not seem to increase performance as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-culture",
   "metadata": {},
   "source": [
    "# Option 4:  10 layer Convolutional net with with L2 regularisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medical-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional3( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional3, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            #layer 1\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #layer 2\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #layer 3\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #layer 4\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #layer 5\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #layer 6\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #layer 7\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #layer 8\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #layer 9\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Output process \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "recreational-settle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.693 acc: 0.498 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 2 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 3 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 4 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 5 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 6 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 7 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 8 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 9 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 10 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 11 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 12 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 13 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 14 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 15 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 16 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 17 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 18 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 19 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 20 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 21 loss: 0.692 acc: 0.520 val_loss: 0.691 val_acc: 0.523\n",
      "Epoch 22 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 23 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 24 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 25 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 26 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 27 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 28 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 29 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 30 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 31 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 32 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 33 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 34 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 35 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 36 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 37 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 38 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 39 loss: 0.692 acc: 0.520 val_loss: 0.691 val_acc: 0.523\n",
      "Epoch 40 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 41 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 42 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 43 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 44 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 45 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 46 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 47 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 48 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 49 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 50 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 51 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 52 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 53 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 54 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 55 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 56 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 57 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 58 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 59 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 60 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 61 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 62 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 63 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 64 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 65 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 66 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 67 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 68 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 69 loss: 0.692 acc: 0.520 val_loss: 0.691 val_acc: 0.523\n",
      "Epoch 70 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 71 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 72 loss: 0.693 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 73 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 74 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 75 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 76 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 77 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 78 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 79 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 80 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 81 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 82 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 83 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 84 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 85 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 86 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 87 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 88 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 89 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 90 loss: 0.692 acc: 0.520 val_loss: 0.691 val_acc: 0.523\n",
      "Epoch 91 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 92 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 93 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 94 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 95 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 96 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 97 loss: 0.692 acc: 0.520 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 98 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 99 loss: 0.693 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 100 loss: 0.692 acc: 0.520 val_loss: 0.692 val_acc: 0.523\n"
     ]
    }
   ],
   "source": [
    "history_4, net_4 = define_and_train(NN_convolutional3, 100, training_set, test_set, batch_size=32, weight_decay= ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-injection",
   "metadata": {},
   "source": [
    "10 layers gets stuck in weird local minima, takes a while to train even on a dataset of 1000 and performance is awful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-sunrise",
   "metadata": {},
   "source": [
    "## Option 6: Basic NN with batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adult-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional4( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional4, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "graduate-madness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.686 acc: 0.545 val_loss: 0.665 val_acc: 0.631\n",
      "Epoch 2 loss: 0.611 acc: 0.752 val_loss: 0.589 val_acc: 0.703\n",
      "Epoch 3 loss: 0.498 acc: 0.801 val_loss: 0.524 val_acc: 0.748\n",
      "Epoch 4 loss: 0.426 acc: 0.812 val_loss: 0.499 val_acc: 0.747\n",
      "Epoch 5 loss: 0.389 acc: 0.832 val_loss: 0.506 val_acc: 0.745\n",
      "Epoch 6 loss: 0.345 acc: 0.854 val_loss: 0.493 val_acc: 0.761\n",
      "Epoch 7 loss: 0.300 acc: 0.884 val_loss: 0.488 val_acc: 0.764\n",
      "Epoch 8 loss: 0.263 acc: 0.907 val_loss: 0.513 val_acc: 0.765\n",
      "Epoch 9 loss: 0.227 acc: 0.919 val_loss: 0.506 val_acc: 0.776\n",
      "Epoch 10 loss: 0.202 acc: 0.931 val_loss: 0.527 val_acc: 0.771\n",
      "Epoch 11 loss: 0.173 acc: 0.933 val_loss: 0.555 val_acc: 0.764\n",
      "Epoch 12 loss: 0.154 acc: 0.944 val_loss: 0.546 val_acc: 0.769\n",
      "Epoch 13 loss: 0.127 acc: 0.961 val_loss: 0.614 val_acc: 0.760\n",
      "Epoch 14 loss: 0.110 acc: 0.968 val_loss: 0.595 val_acc: 0.766\n",
      "Epoch 15 loss: 0.095 acc: 0.970 val_loss: 0.695 val_acc: 0.747\n",
      "Epoch 16 loss: 0.075 acc: 0.979 val_loss: 0.713 val_acc: 0.751\n",
      "Epoch 17 loss: 0.047 acc: 0.994 val_loss: 0.698 val_acc: 0.765\n",
      "Epoch 18 loss: 0.049 acc: 0.993 val_loss: 0.744 val_acc: 0.747\n",
      "Epoch 19 loss: 0.048 acc: 0.990 val_loss: 0.776 val_acc: 0.752\n",
      "Epoch 20 loss: 0.042 acc: 0.994 val_loss: 0.781 val_acc: 0.737\n",
      "Epoch 21 loss: 0.032 acc: 0.998 val_loss: 0.817 val_acc: 0.749\n",
      "Epoch 22 loss: 0.028 acc: 0.996 val_loss: 0.813 val_acc: 0.753\n",
      "Epoch 23 loss: 0.035 acc: 0.996 val_loss: 0.937 val_acc: 0.732\n",
      "Epoch 24 loss: 0.056 acc: 0.985 val_loss: 0.920 val_acc: 0.726\n",
      "Epoch 25 loss: 0.056 acc: 0.985 val_loss: 0.915 val_acc: 0.746\n",
      "Epoch 26 loss: 0.048 acc: 0.988 val_loss: 0.871 val_acc: 0.754\n",
      "Epoch 27 loss: 0.027 acc: 0.995 val_loss: 0.907 val_acc: 0.758\n",
      "Epoch 28 loss: 0.026 acc: 0.996 val_loss: 0.939 val_acc: 0.737\n",
      "Epoch 29 loss: 0.041 acc: 0.991 val_loss: 0.984 val_acc: 0.739\n",
      "Epoch 30 loss: 0.039 acc: 0.988 val_loss: 0.984 val_acc: 0.734\n",
      "Epoch 31 loss: 0.029 acc: 0.992 val_loss: 1.038 val_acc: 0.749\n",
      "Epoch 32 loss: 0.039 acc: 0.995 val_loss: 1.042 val_acc: 0.736\n",
      "Epoch 33 loss: 0.066 acc: 0.975 val_loss: 1.057 val_acc: 0.737\n",
      "Epoch 34 loss: 0.028 acc: 0.994 val_loss: 1.037 val_acc: 0.740\n",
      "Epoch 35 loss: 0.019 acc: 0.995 val_loss: 1.135 val_acc: 0.740\n",
      "Epoch 36 loss: 0.023 acc: 0.997 val_loss: 1.108 val_acc: 0.742\n",
      "Epoch 37 loss: 0.029 acc: 0.990 val_loss: 1.090 val_acc: 0.738\n",
      "Epoch 38 loss: 0.056 acc: 0.995 val_loss: 1.143 val_acc: 0.737\n",
      "Epoch 39 loss: 0.049 acc: 0.978 val_loss: 1.018 val_acc: 0.747\n",
      "Epoch 40 loss: 0.021 acc: 0.994 val_loss: 1.109 val_acc: 0.743\n",
      "Epoch 41 loss: 0.011 acc: 0.998 val_loss: 1.100 val_acc: 0.737\n",
      "Epoch 42 loss: 0.007 acc: 1.000 val_loss: 1.099 val_acc: 0.744\n",
      "Epoch 43 loss: 0.007 acc: 1.000 val_loss: 1.106 val_acc: 0.739\n",
      "Epoch 44 loss: 0.008 acc: 0.999 val_loss: 1.141 val_acc: 0.739\n",
      "Epoch 45 loss: 0.005 acc: 0.999 val_loss: 1.152 val_acc: 0.723\n",
      "Epoch 46 loss: 0.004 acc: 1.000 val_loss: 1.135 val_acc: 0.739\n",
      "Epoch 47 loss: 0.004 acc: 1.000 val_loss: 1.185 val_acc: 0.738\n",
      "Epoch 48 loss: 0.004 acc: 1.000 val_loss: 1.164 val_acc: 0.745\n",
      "Epoch 49 loss: 0.002 acc: 1.000 val_loss: 1.164 val_acc: 0.750\n",
      "Epoch 50 loss: 0.003 acc: 1.000 val_loss: 1.233 val_acc: 0.740\n",
      "Epoch 51 loss: 0.003 acc: 1.000 val_loss: 1.198 val_acc: 0.736\n",
      "Epoch 52 loss: 0.002 acc: 1.000 val_loss: 1.136 val_acc: 0.749\n",
      "Epoch 53 loss: 0.003 acc: 1.000 val_loss: 1.243 val_acc: 0.736\n",
      "Epoch 54 loss: 0.002 acc: 1.000 val_loss: 1.325 val_acc: 0.744\n",
      "Epoch 55 loss: 0.002 acc: 1.000 val_loss: 1.244 val_acc: 0.740\n",
      "Epoch 56 loss: 0.002 acc: 1.000 val_loss: 1.243 val_acc: 0.738\n",
      "Epoch 57 loss: 0.002 acc: 1.000 val_loss: 1.238 val_acc: 0.748\n",
      "Epoch 58 loss: 0.001 acc: 1.000 val_loss: 1.192 val_acc: 0.744\n",
      "Epoch 59 loss: 0.002 acc: 1.000 val_loss: 1.186 val_acc: 0.752\n",
      "Epoch 60 loss: 0.002 acc: 1.000 val_loss: 1.243 val_acc: 0.740\n",
      "Epoch 61 loss: 0.001 acc: 1.000 val_loss: 1.321 val_acc: 0.734\n",
      "Epoch 62 loss: 0.002 acc: 1.000 val_loss: 1.368 val_acc: 0.740\n",
      "Epoch 63 loss: 0.001 acc: 1.000 val_loss: 1.379 val_acc: 0.740\n",
      "Epoch 64 loss: 0.001 acc: 1.000 val_loss: 1.416 val_acc: 0.731\n",
      "Epoch 65 loss: 0.001 acc: 1.000 val_loss: 1.401 val_acc: 0.741\n",
      "Epoch 66 loss: 0.001 acc: 1.000 val_loss: 1.296 val_acc: 0.748\n",
      "Epoch 67 loss: 0.001 acc: 1.000 val_loss: 1.368 val_acc: 0.740\n",
      "Epoch 68 loss: 0.001 acc: 1.000 val_loss: 1.409 val_acc: 0.735\n",
      "Epoch 69 loss: 0.001 acc: 1.000 val_loss: 1.372 val_acc: 0.748\n",
      "Epoch 70 loss: 0.001 acc: 1.000 val_loss: 1.413 val_acc: 0.742\n",
      "Epoch 71 loss: 0.002 acc: 1.000 val_loss: 1.390 val_acc: 0.738\n",
      "Epoch 72 loss: 0.002 acc: 1.000 val_loss: 1.441 val_acc: 0.740\n",
      "Epoch 73 loss: 0.002 acc: 0.999 val_loss: 1.327 val_acc: 0.744\n",
      "Epoch 74 loss: 0.002 acc: 1.000 val_loss: 1.337 val_acc: 0.747\n",
      "Epoch 75 loss: 0.001 acc: 1.000 val_loss: 1.315 val_acc: 0.754\n",
      "Epoch 76 loss: 0.001 acc: 1.000 val_loss: 1.482 val_acc: 0.739\n",
      "Epoch 77 loss: 0.001 acc: 1.000 val_loss: 1.492 val_acc: 0.753\n",
      "Epoch 78 loss: 0.001 acc: 1.000 val_loss: 1.523 val_acc: 0.752\n",
      "Epoch 79 loss: 0.001 acc: 1.000 val_loss: 1.500 val_acc: 0.743\n",
      "Epoch 80 loss: 0.001 acc: 1.000 val_loss: 1.424 val_acc: 0.749\n",
      "Epoch 81 loss: 0.001 acc: 1.000 val_loss: 1.436 val_acc: 0.750\n",
      "Epoch 82 loss: 0.000 acc: 1.000 val_loss: 1.366 val_acc: 0.758\n",
      "Epoch 83 loss: 0.001 acc: 1.000 val_loss: 1.417 val_acc: 0.742\n",
      "Epoch 84 loss: 0.002 acc: 1.000 val_loss: 1.416 val_acc: 0.746\n",
      "Epoch 85 loss: 0.005 acc: 0.999 val_loss: 1.583 val_acc: 0.732\n",
      "Epoch 86 loss: 0.004 acc: 0.999 val_loss: 1.461 val_acc: 0.746\n",
      "Epoch 87 loss: 0.007 acc: 0.998 val_loss: 1.814 val_acc: 0.744\n",
      "Epoch 88 loss: 0.041 acc: 0.995 val_loss: 1.761 val_acc: 0.743\n",
      "Epoch 89 loss: 0.193 acc: 0.932 val_loss: 1.302 val_acc: 0.743\n",
      "Epoch 90 loss: 0.123 acc: 0.953 val_loss: 1.132 val_acc: 0.750\n",
      "Epoch 91 loss: 0.047 acc: 0.981 val_loss: 1.082 val_acc: 0.756\n",
      "Epoch 92 loss: 0.019 acc: 0.993 val_loss: 1.125 val_acc: 0.758\n",
      "Epoch 93 loss: 0.006 acc: 1.000 val_loss: 1.108 val_acc: 0.756\n",
      "Epoch 94 loss: 0.007 acc: 0.998 val_loss: 1.123 val_acc: 0.762\n",
      "Epoch 95 loss: 0.009 acc: 0.999 val_loss: 1.167 val_acc: 0.769\n",
      "Epoch 96 loss: 0.010 acc: 0.997 val_loss: 1.202 val_acc: 0.745\n",
      "Epoch 97 loss: 0.006 acc: 1.000 val_loss: 1.262 val_acc: 0.746\n",
      "Epoch 98 loss: 0.005 acc: 0.999 val_loss: 1.265 val_acc: 0.750\n",
      "Epoch 99 loss: 0.002 acc: 1.000 val_loss: 1.204 val_acc: 0.752\n",
      "Epoch 100 loss: 0.002 acc: 1.000 val_loss: 1.276 val_acc: 0.754\n"
     ]
    }
   ],
   "source": [
    "history_6, net_6 = define_and_train(NN_convolutional4, 100, training_set, test_set, batch_size=32, weight_decay= 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-while",
   "metadata": {},
   "source": [
    "Batch norm starts off promisingly but then flattens out! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-nation",
   "metadata": {},
   "source": [
    "## Option 7: 5 layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "neural-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional5( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional5, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            #layer 1\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #layer 2\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #layer 3\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #layer 4\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #layer 5\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            #Output process \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "still-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.694 acc: 0.493 val_loss: 0.695 val_acc: 0.482\n",
      "Epoch 2 loss: 0.694 acc: 0.493 val_loss: 0.694 val_acc: 0.482\n",
      "Epoch 3 loss: 0.693 acc: 0.493 val_loss: 0.694 val_acc: 0.482\n",
      "Epoch 4 loss: 0.693 acc: 0.493 val_loss: 0.694 val_acc: 0.482\n",
      "Epoch 5 loss: 0.693 acc: 0.493 val_loss: 0.693 val_acc: 0.482\n",
      "Epoch 6 loss: 0.693 acc: 0.493 val_loss: 0.693 val_acc: 0.482\n",
      "Epoch 7 loss: 0.693 acc: 0.493 val_loss: 0.693 val_acc: 0.482\n",
      "Epoch 8 loss: 0.693 acc: 0.497 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 9 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 10 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 11 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 12 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 13 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 14 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 15 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 16 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 17 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 18 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 19 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 20 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 21 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 22 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.518\n",
      "Epoch 23 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 24 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 25 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 26 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 27 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 28 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 29 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 30 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 31 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 32 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 33 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 34 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 35 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 36 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 37 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 38 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 39 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 40 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 41 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 42 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 43 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 44 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 45 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 46 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 47 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 48 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 49 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 50 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 51 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 52 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 53 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 54 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 55 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 56 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 57 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 58 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 59 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 60 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 61 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 62 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 63 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 64 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 65 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 66 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 67 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 68 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 69 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 70 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 71 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 72 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 73 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 74 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 75 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 76 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 77 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 78 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 79 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 80 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 81 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 82 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 83 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 84 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 85 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 86 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 87 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 88 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 89 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 90 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 91 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 92 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 93 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 94 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 95 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 96 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 97 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 98 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 99 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n",
      "Epoch 100 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.518\n"
     ]
    }
   ],
   "source": [
    "history_7, net_7 = define_and_train(NN_convolutional5, 100, training_set, test_set, batch_size=32, weight_decay= 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-valuable",
   "metadata": {},
   "source": [
    "5 layer network without regularisation seems tempermental. Seems to get stuck in local minima more often than not! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-taylor",
   "metadata": {},
   "source": [
    "# Option 8: 5 layer CNN with L2 regularisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "chief-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.698 acc: 0.507 val_loss: 0.694 val_acc: 0.518\n",
      "Epoch 2 loss: 0.695 acc: 0.507 val_loss: 0.692 val_acc: 0.518\n",
      "Epoch 3 loss: 0.693 acc: 0.505 val_loss: 0.691 val_acc: 0.518\n",
      "Epoch 4 loss: 0.689 acc: 0.522 val_loss: 0.692 val_acc: 0.484\n",
      "Epoch 5 loss: 0.678 acc: 0.572 val_loss: 0.648 val_acc: 0.620\n",
      "Epoch 6 loss: 0.626 acc: 0.676 val_loss: 0.747 val_acc: 0.601\n",
      "Epoch 7 loss: 0.631 acc: 0.667 val_loss: 0.638 val_acc: 0.646\n",
      "Epoch 8 loss: 0.601 acc: 0.719 val_loss: 0.613 val_acc: 0.699\n",
      "Epoch 9 loss: 0.604 acc: 0.709 val_loss: 0.607 val_acc: 0.722\n",
      "Epoch 10 loss: 0.599 acc: 0.709 val_loss: 0.591 val_acc: 0.725\n",
      "Epoch 11 loss: 0.581 acc: 0.744 val_loss: 0.599 val_acc: 0.712\n",
      "Epoch 12 loss: 0.601 acc: 0.693 val_loss: 0.578 val_acc: 0.742\n",
      "Epoch 13 loss: 0.558 acc: 0.763 val_loss: 0.574 val_acc: 0.743\n",
      "Epoch 14 loss: 0.552 acc: 0.781 val_loss: 0.553 val_acc: 0.766\n",
      "Epoch 15 loss: 0.530 acc: 0.766 val_loss: 0.534 val_acc: 0.772\n",
      "Epoch 16 loss: 0.517 acc: 0.776 val_loss: 0.587 val_acc: 0.746\n",
      "Epoch 17 loss: 0.512 acc: 0.783 val_loss: 0.501 val_acc: 0.787\n",
      "Epoch 18 loss: 0.536 acc: 0.762 val_loss: 0.503 val_acc: 0.780\n",
      "Epoch 19 loss: 0.487 acc: 0.801 val_loss: 0.476 val_acc: 0.792\n",
      "Epoch 20 loss: 0.464 acc: 0.813 val_loss: 0.493 val_acc: 0.798\n",
      "Epoch 21 loss: 0.422 acc: 0.834 val_loss: 0.439 val_acc: 0.822\n",
      "Epoch 22 loss: 0.387 acc: 0.862 val_loss: 0.446 val_acc: 0.815\n",
      "Epoch 23 loss: 0.357 acc: 0.856 val_loss: 0.385 val_acc: 0.842\n",
      "Epoch 24 loss: 0.321 acc: 0.878 val_loss: 0.378 val_acc: 0.844\n",
      "Epoch 25 loss: 0.311 acc: 0.876 val_loss: 0.360 val_acc: 0.857\n",
      "Epoch 26 loss: 0.276 acc: 0.900 val_loss: 0.384 val_acc: 0.859\n",
      "Epoch 27 loss: 0.260 acc: 0.893 val_loss: 0.333 val_acc: 0.857\n",
      "Epoch 28 loss: 0.221 acc: 0.915 val_loss: 0.337 val_acc: 0.866\n",
      "Epoch 29 loss: 0.211 acc: 0.927 val_loss: 0.330 val_acc: 0.862\n",
      "Epoch 30 loss: 0.198 acc: 0.917 val_loss: 0.320 val_acc: 0.872\n",
      "Epoch 31 loss: 0.193 acc: 0.921 val_loss: 0.274 val_acc: 0.877\n",
      "Epoch 32 loss: 0.173 acc: 0.928 val_loss: 0.274 val_acc: 0.882\n",
      "Epoch 33 loss: 0.179 acc: 0.928 val_loss: 0.274 val_acc: 0.885\n",
      "Epoch 34 loss: 0.204 acc: 0.916 val_loss: 0.308 val_acc: 0.886\n",
      "Epoch 35 loss: 0.154 acc: 0.935 val_loss: 0.271 val_acc: 0.886\n",
      "Epoch 36 loss: 0.143 acc: 0.945 val_loss: 0.417 val_acc: 0.855\n",
      "Epoch 37 loss: 0.154 acc: 0.935 val_loss: 0.252 val_acc: 0.891\n",
      "Epoch 38 loss: 0.122 acc: 0.960 val_loss: 0.256 val_acc: 0.895\n",
      "Epoch 39 loss: 0.121 acc: 0.954 val_loss: 0.254 val_acc: 0.889\n",
      "Epoch 40 loss: 0.108 acc: 0.959 val_loss: 0.270 val_acc: 0.893\n",
      "Epoch 41 loss: 0.114 acc: 0.956 val_loss: 0.267 val_acc: 0.890\n",
      "Epoch 42 loss: 0.094 acc: 0.963 val_loss: 0.269 val_acc: 0.894\n",
      "Epoch 43 loss: 0.100 acc: 0.961 val_loss: 0.249 val_acc: 0.897\n",
      "Epoch 44 loss: 0.105 acc: 0.959 val_loss: 0.266 val_acc: 0.897\n",
      "Epoch 45 loss: 0.099 acc: 0.963 val_loss: 0.274 val_acc: 0.901\n",
      "Epoch 46 loss: 0.082 acc: 0.965 val_loss: 0.286 val_acc: 0.892\n",
      "Epoch 47 loss: 0.087 acc: 0.963 val_loss: 0.307 val_acc: 0.898\n",
      "Epoch 48 loss: 0.078 acc: 0.969 val_loss: 0.313 val_acc: 0.894\n",
      "Epoch 49 loss: 0.069 acc: 0.975 val_loss: 0.274 val_acc: 0.898\n",
      "Epoch 50 loss: 0.054 acc: 0.982 val_loss: 0.283 val_acc: 0.898\n",
      "Epoch 51 loss: 0.048 acc: 0.985 val_loss: 0.306 val_acc: 0.896\n",
      "Epoch 52 loss: 0.055 acc: 0.979 val_loss: 0.359 val_acc: 0.896\n",
      "Epoch 53 loss: 0.062 acc: 0.983 val_loss: 0.325 val_acc: 0.892\n",
      "Epoch 54 loss: 0.042 acc: 0.988 val_loss: 0.343 val_acc: 0.887\n",
      "Epoch 55 loss: 0.040 acc: 0.990 val_loss: 0.349 val_acc: 0.894\n",
      "Epoch 56 loss: 0.044 acc: 0.985 val_loss: 0.363 val_acc: 0.896\n",
      "Epoch 57 loss: 0.030 acc: 0.994 val_loss: 0.369 val_acc: 0.902\n",
      "Epoch 58 loss: 0.034 acc: 0.986 val_loss: 0.356 val_acc: 0.901\n",
      "Epoch 59 loss: 0.025 acc: 0.994 val_loss: 0.368 val_acc: 0.896\n",
      "Epoch 60 loss: 0.024 acc: 0.996 val_loss: 0.371 val_acc: 0.908\n",
      "Epoch 61 loss: 0.021 acc: 0.996 val_loss: 0.396 val_acc: 0.901\n",
      "Epoch 62 loss: 0.019 acc: 0.995 val_loss: 0.385 val_acc: 0.899\n",
      "Epoch 63 loss: 0.026 acc: 0.992 val_loss: 0.466 val_acc: 0.896\n",
      "Epoch 64 loss: 0.072 acc: 0.969 val_loss: 0.428 val_acc: 0.904\n",
      "Epoch 65 loss: 0.020 acc: 0.996 val_loss: 0.384 val_acc: 0.903\n",
      "Epoch 66 loss: 0.019 acc: 0.994 val_loss: 0.384 val_acc: 0.896\n",
      "Epoch 67 loss: 0.022 acc: 0.993 val_loss: 0.393 val_acc: 0.907\n",
      "Epoch 68 loss: 0.016 acc: 0.996 val_loss: 0.429 val_acc: 0.906\n",
      "Epoch 69 loss: 0.008 acc: 1.000 val_loss: 0.431 val_acc: 0.907\n",
      "Epoch 70 loss: 0.010 acc: 0.998 val_loss: 0.421 val_acc: 0.903\n",
      "Epoch 71 loss: 0.007 acc: 0.999 val_loss: 0.448 val_acc: 0.905\n",
      "Epoch 72 loss: 0.006 acc: 1.000 val_loss: 0.453 val_acc: 0.903\n",
      "Epoch 73 loss: 0.006 acc: 1.000 val_loss: 0.488 val_acc: 0.906\n",
      "Epoch 74 loss: 0.040 acc: 0.984 val_loss: 0.457 val_acc: 0.897\n",
      "Epoch 75 loss: 0.116 acc: 0.960 val_loss: 0.461 val_acc: 0.893\n",
      "Epoch 76 loss: 0.029 acc: 0.995 val_loss: 0.378 val_acc: 0.910\n",
      "Epoch 77 loss: 0.011 acc: 1.000 val_loss: 0.392 val_acc: 0.912\n",
      "Epoch 78 loss: 0.008 acc: 0.999 val_loss: 0.419 val_acc: 0.910\n",
      "Epoch 79 loss: 0.005 acc: 1.000 val_loss: 0.435 val_acc: 0.908\n",
      "Epoch 80 loss: 0.005 acc: 1.000 val_loss: 0.435 val_acc: 0.909\n",
      "Epoch 81 loss: 0.004 acc: 1.000 val_loss: 0.464 val_acc: 0.913\n",
      "Epoch 82 loss: 0.003 acc: 1.000 val_loss: 0.472 val_acc: 0.909\n",
      "Epoch 83 loss: 0.003 acc: 1.000 val_loss: 0.508 val_acc: 0.914\n",
      "Epoch 84 loss: 0.002 acc: 1.000 val_loss: 0.523 val_acc: 0.910\n",
      "Epoch 85 loss: 0.002 acc: 1.000 val_loss: 0.488 val_acc: 0.911\n",
      "Epoch 86 loss: 0.002 acc: 1.000 val_loss: 0.581 val_acc: 0.913\n",
      "Epoch 87 loss: 0.002 acc: 1.000 val_loss: 0.585 val_acc: 0.913\n",
      "Epoch 88 loss: 0.002 acc: 1.000 val_loss: 0.766 val_acc: 0.906\n",
      "Epoch 89 loss: 0.002 acc: 1.000 val_loss: 0.705 val_acc: 0.911\n",
      "Epoch 90 loss: 0.001 acc: 1.000 val_loss: 0.682 val_acc: 0.911\n",
      "Epoch 91 loss: 0.001 acc: 1.000 val_loss: 0.693 val_acc: 0.910\n",
      "Epoch 92 loss: 0.001 acc: 1.000 val_loss: 0.715 val_acc: 0.910\n",
      "Epoch 93 loss: 0.001 acc: 1.000 val_loss: 0.730 val_acc: 0.910\n",
      "Epoch 94 loss: 0.001 acc: 1.000 val_loss: 0.750 val_acc: 0.910\n",
      "Epoch 95 loss: 0.001 acc: 1.000 val_loss: 0.796 val_acc: 0.910\n",
      "Epoch 96 loss: 0.001 acc: 1.000 val_loss: 0.733 val_acc: 0.909\n",
      "Epoch 97 loss: 0.001 acc: 1.000 val_loss: 0.721 val_acc: 0.910\n",
      "Epoch 98 loss: 0.001 acc: 1.000 val_loss: 0.810 val_acc: 0.908\n",
      "Epoch 99 loss: 0.001 acc: 1.000 val_loss: 0.813 val_acc: 0.909\n",
      "Epoch 100 loss: 0.001 acc: 1.000 val_loss: 1.111 val_acc: 0.909\n"
     ]
    }
   ],
   "source": [
    "history_8, net_8 = define_and_train(NN_convolutional5, 100, training_set, test_set, batch_size=32, weight_decay= 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-semiconductor",
   "metadata": {},
   "source": [
    "5 layer with regularisation seems to help, does not get stuck in local minima! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-westminster",
   "metadata": {},
   "source": [
    "# Option 9: 5 layer CNN with batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "altered-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional6( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional6, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(12),\n",
    "            #layer 1\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            #layer 2\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            #layer 3\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            #layer 4\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            #layer 5\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2),\n",
    "            #Output process \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "manual-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.680 acc: 0.576 val_loss: 0.650 val_acc: 0.667\n",
      "Epoch 2 loss: 0.564 acc: 0.764 val_loss: 0.535 val_acc: 0.749\n",
      "Epoch 3 loss: 0.437 acc: 0.828 val_loss: 0.500 val_acc: 0.760\n",
      "Epoch 4 loss: 0.339 acc: 0.884 val_loss: 0.540 val_acc: 0.744\n",
      "Epoch 5 loss: 0.232 acc: 0.919 val_loss: 0.565 val_acc: 0.758\n",
      "Epoch 6 loss: 0.151 acc: 0.954 val_loss: 0.618 val_acc: 0.759\n",
      "Epoch 7 loss: 0.093 acc: 0.980 val_loss: 0.629 val_acc: 0.765\n",
      "Epoch 8 loss: 0.066 acc: 0.986 val_loss: 0.687 val_acc: 0.756\n",
      "Epoch 9 loss: 0.054 acc: 0.987 val_loss: 0.739 val_acc: 0.751\n",
      "Epoch 10 loss: 0.046 acc: 0.985 val_loss: 0.797 val_acc: 0.772\n",
      "Epoch 11 loss: 0.042 acc: 0.990 val_loss: 0.763 val_acc: 0.769\n",
      "Epoch 12 loss: 0.021 acc: 0.997 val_loss: 0.968 val_acc: 0.757\n",
      "Epoch 13 loss: 0.020 acc: 0.994 val_loss: 0.858 val_acc: 0.753\n",
      "Epoch 14 loss: 0.042 acc: 0.988 val_loss: 0.888 val_acc: 0.767\n",
      "Epoch 15 loss: 0.030 acc: 0.991 val_loss: 0.912 val_acc: 0.778\n",
      "Epoch 16 loss: 0.045 acc: 0.983 val_loss: 1.002 val_acc: 0.765\n",
      "Epoch 17 loss: 0.036 acc: 0.986 val_loss: 0.997 val_acc: 0.759\n",
      "Epoch 18 loss: 0.083 acc: 0.968 val_loss: 0.954 val_acc: 0.747\n",
      "Epoch 19 loss: 0.091 acc: 0.968 val_loss: 0.915 val_acc: 0.749\n",
      "Epoch 20 loss: 0.082 acc: 0.969 val_loss: 0.916 val_acc: 0.733\n",
      "Epoch 21 loss: 0.054 acc: 0.982 val_loss: 0.872 val_acc: 0.756\n",
      "Epoch 22 loss: 0.019 acc: 0.994 val_loss: 0.881 val_acc: 0.775\n",
      "Epoch 23 loss: 0.009 acc: 0.998 val_loss: 0.913 val_acc: 0.771\n",
      "Epoch 24 loss: 0.005 acc: 1.000 val_loss: 0.892 val_acc: 0.784\n",
      "Epoch 25 loss: 0.003 acc: 1.000 val_loss: 0.926 val_acc: 0.789\n",
      "Epoch 26 loss: 0.003 acc: 1.000 val_loss: 0.970 val_acc: 0.770\n",
      "Epoch 27 loss: 0.004 acc: 1.000 val_loss: 0.940 val_acc: 0.787\n",
      "Epoch 28 loss: 0.002 acc: 1.000 val_loss: 0.970 val_acc: 0.768\n",
      "Epoch 29 loss: 0.001 acc: 1.000 val_loss: 0.992 val_acc: 0.776\n",
      "Epoch 30 loss: 0.001 acc: 1.000 val_loss: 0.964 val_acc: 0.784\n",
      "Epoch 31 loss: 0.001 acc: 1.000 val_loss: 1.049 val_acc: 0.783\n",
      "Epoch 32 loss: 0.001 acc: 1.000 val_loss: 0.998 val_acc: 0.781\n",
      "Epoch 33 loss: 0.001 acc: 1.000 val_loss: 1.032 val_acc: 0.785\n",
      "Epoch 34 loss: 0.001 acc: 1.000 val_loss: 1.000 val_acc: 0.784\n",
      "Epoch 35 loss: 0.001 acc: 1.000 val_loss: 1.070 val_acc: 0.768\n",
      "Epoch 36 loss: 0.000 acc: 1.000 val_loss: 1.022 val_acc: 0.776\n",
      "Epoch 37 loss: 0.001 acc: 1.000 val_loss: 1.092 val_acc: 0.767\n",
      "Epoch 38 loss: 0.001 acc: 1.000 val_loss: 1.078 val_acc: 0.776\n",
      "Epoch 39 loss: 0.000 acc: 1.000 val_loss: 1.084 val_acc: 0.765\n",
      "Epoch 40 loss: 0.001 acc: 1.000 val_loss: 1.082 val_acc: 0.767\n",
      "Epoch 41 loss: 0.000 acc: 1.000 val_loss: 1.074 val_acc: 0.774\n",
      "Epoch 42 loss: 0.000 acc: 1.000 val_loss: 1.123 val_acc: 0.776\n",
      "Epoch 43 loss: 0.000 acc: 1.000 val_loss: 1.195 val_acc: 0.773\n",
      "Epoch 44 loss: 0.000 acc: 1.000 val_loss: 1.093 val_acc: 0.775\n",
      "Epoch 45 loss: 0.000 acc: 1.000 val_loss: 1.073 val_acc: 0.783\n",
      "Epoch 46 loss: 0.000 acc: 1.000 val_loss: 1.056 val_acc: 0.781\n",
      "Epoch 47 loss: 0.001 acc: 1.000 val_loss: 1.118 val_acc: 0.779\n",
      "Epoch 48 loss: 0.000 acc: 1.000 val_loss: 1.153 val_acc: 0.771\n",
      "Epoch 49 loss: 0.000 acc: 1.000 val_loss: 1.103 val_acc: 0.792\n",
      "Epoch 50 loss: 0.000 acc: 1.000 val_loss: 1.231 val_acc: 0.775\n",
      "Epoch 51 loss: 0.000 acc: 1.000 val_loss: 1.130 val_acc: 0.779\n",
      "Epoch 52 loss: 0.000 acc: 1.000 val_loss: 1.142 val_acc: 0.784\n",
      "Epoch 53 loss: 0.006 acc: 0.999 val_loss: 1.247 val_acc: 0.750\n",
      "Epoch 54 loss: 0.184 acc: 0.935 val_loss: 1.090 val_acc: 0.739\n",
      "Epoch 55 loss: 0.309 acc: 0.891 val_loss: 0.640 val_acc: 0.756\n",
      "Epoch 56 loss: 0.087 acc: 0.971 val_loss: 0.665 val_acc: 0.786\n",
      "Epoch 57 loss: 0.031 acc: 0.995 val_loss: 0.764 val_acc: 0.778\n",
      "Epoch 58 loss: 0.033 acc: 0.990 val_loss: 0.845 val_acc: 0.775\n",
      "Epoch 59 loss: 0.021 acc: 0.993 val_loss: 0.830 val_acc: 0.784\n",
      "Epoch 60 loss: 0.017 acc: 0.997 val_loss: 0.880 val_acc: 0.779\n",
      "Epoch 61 loss: 0.022 acc: 0.992 val_loss: 0.873 val_acc: 0.767\n",
      "Epoch 62 loss: 0.016 acc: 0.996 val_loss: 0.944 val_acc: 0.770\n",
      "Epoch 63 loss: 0.016 acc: 0.993 val_loss: 1.044 val_acc: 0.746\n",
      "Epoch 64 loss: 0.013 acc: 0.995 val_loss: 0.872 val_acc: 0.800\n",
      "Epoch 65 loss: 0.009 acc: 0.997 val_loss: 0.944 val_acc: 0.784\n",
      "Epoch 66 loss: 0.021 acc: 0.994 val_loss: 0.915 val_acc: 0.802\n",
      "Epoch 67 loss: 0.021 acc: 0.994 val_loss: 0.972 val_acc: 0.772\n",
      "Epoch 68 loss: 0.012 acc: 0.996 val_loss: 1.067 val_acc: 0.771\n",
      "Epoch 69 loss: 0.006 acc: 0.997 val_loss: 1.100 val_acc: 0.752\n",
      "Epoch 70 loss: 0.007 acc: 0.998 val_loss: 1.022 val_acc: 0.765\n",
      "Epoch 71 loss: 0.005 acc: 0.999 val_loss: 1.066 val_acc: 0.774\n",
      "Epoch 72 loss: 0.003 acc: 0.999 val_loss: 1.025 val_acc: 0.770\n",
      "Epoch 73 loss: 0.003 acc: 0.999 val_loss: 1.067 val_acc: 0.779\n",
      "Epoch 74 loss: 0.001 acc: 1.000 val_loss: 1.077 val_acc: 0.786\n",
      "Epoch 75 loss: 0.001 acc: 1.000 val_loss: 1.136 val_acc: 0.780\n",
      "Epoch 76 loss: 0.000 acc: 1.000 val_loss: 1.076 val_acc: 0.785\n",
      "Epoch 77 loss: 0.000 acc: 1.000 val_loss: 1.138 val_acc: 0.784\n",
      "Epoch 78 loss: 0.001 acc: 1.000 val_loss: 1.106 val_acc: 0.786\n",
      "Epoch 79 loss: 0.002 acc: 1.000 val_loss: 1.146 val_acc: 0.773\n",
      "Epoch 80 loss: 0.000 acc: 1.000 val_loss: 1.149 val_acc: 0.772\n",
      "Epoch 81 loss: 0.001 acc: 1.000 val_loss: 1.148 val_acc: 0.780\n",
      "Epoch 82 loss: 0.001 acc: 1.000 val_loss: 1.282 val_acc: 0.759\n",
      "Epoch 83 loss: 0.000 acc: 1.000 val_loss: 1.206 val_acc: 0.760\n",
      "Epoch 84 loss: 0.001 acc: 1.000 val_loss: 1.196 val_acc: 0.772\n",
      "Epoch 85 loss: 0.000 acc: 1.000 val_loss: 1.206 val_acc: 0.773\n",
      "Epoch 86 loss: 0.000 acc: 1.000 val_loss: 1.140 val_acc: 0.777\n",
      "Epoch 87 loss: 0.000 acc: 1.000 val_loss: 1.175 val_acc: 0.782\n",
      "Epoch 88 loss: 0.000 acc: 1.000 val_loss: 1.235 val_acc: 0.780\n",
      "Epoch 89 loss: 0.001 acc: 1.000 val_loss: 1.249 val_acc: 0.778\n",
      "Epoch 90 loss: 0.000 acc: 1.000 val_loss: 1.162 val_acc: 0.777\n",
      "Epoch 91 loss: 0.000 acc: 1.000 val_loss: 1.219 val_acc: 0.765\n",
      "Epoch 92 loss: 0.000 acc: 1.000 val_loss: 1.244 val_acc: 0.779\n",
      "Epoch 93 loss: 0.000 acc: 1.000 val_loss: 1.169 val_acc: 0.783\n",
      "Epoch 94 loss: 0.001 acc: 0.999 val_loss: 1.270 val_acc: 0.761\n",
      "Epoch 95 loss: 0.003 acc: 0.999 val_loss: 1.403 val_acc: 0.766\n",
      "Epoch 96 loss: 0.002 acc: 1.000 val_loss: 1.432 val_acc: 0.785\n",
      "Epoch 97 loss: 0.003 acc: 0.999 val_loss: 1.443 val_acc: 0.767\n",
      "Epoch 98 loss: 0.017 acc: 0.996 val_loss: 1.266 val_acc: 0.777\n",
      "Epoch 99 loss: 0.042 acc: 0.989 val_loss: 1.431 val_acc: 0.778\n",
      "Epoch 100 loss: 0.205 acc: 0.948 val_loss: 1.122 val_acc: 0.729\n"
     ]
    }
   ],
   "source": [
    "history_9, net_9 = define_and_train(NN_convolutional6, 100, training_set, test_set, batch_size=32, weight_decay= 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-robertson",
   "metadata": {},
   "source": [
    "Batch normalistion seems to start off well again but then gets stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "emotional-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.696 acc: 0.517 val_loss: 0.690 val_acc: 0.526\n",
      "Epoch 2 loss: 0.616 acc: 0.719 val_loss: 0.643 val_acc: 0.629\n",
      "Epoch 3 loss: 0.494 acc: 0.798 val_loss: 0.598 val_acc: 0.674\n",
      "Epoch 4 loss: 0.369 acc: 0.850 val_loss: 0.605 val_acc: 0.705\n",
      "Epoch 5 loss: 0.258 acc: 0.915 val_loss: 0.664 val_acc: 0.692\n",
      "Epoch 6 loss: 0.175 acc: 0.958 val_loss: 0.764 val_acc: 0.682\n",
      "Epoch 7 loss: 0.113 acc: 0.972 val_loss: 0.868 val_acc: 0.675\n",
      "Epoch 8 loss: 0.088 acc: 0.971 val_loss: 0.875 val_acc: 0.686\n",
      "Epoch 9 loss: 0.063 acc: 0.986 val_loss: 0.964 val_acc: 0.692\n",
      "Epoch 10 loss: 0.059 acc: 0.982 val_loss: 1.053 val_acc: 0.648\n",
      "Epoch 11 loss: 0.059 acc: 0.982 val_loss: 1.113 val_acc: 0.668\n",
      "Epoch 12 loss: 0.075 acc: 0.975 val_loss: 1.085 val_acc: 0.682\n",
      "Epoch 13 loss: 0.069 acc: 0.980 val_loss: 1.092 val_acc: 0.691\n",
      "Epoch 14 loss: 0.032 acc: 0.991 val_loss: 1.144 val_acc: 0.686\n",
      "Epoch 15 loss: 0.019 acc: 0.997 val_loss: 1.206 val_acc: 0.686\n",
      "Epoch 16 loss: 0.008 acc: 1.000 val_loss: 1.258 val_acc: 0.678\n",
      "Epoch 17 loss: 0.004 acc: 1.000 val_loss: 1.293 val_acc: 0.693\n",
      "Epoch 18 loss: 0.003 acc: 1.000 val_loss: 1.304 val_acc: 0.696\n",
      "Epoch 19 loss: 0.002 acc: 1.000 val_loss: 1.358 val_acc: 0.696\n",
      "Epoch 20 loss: 0.001 acc: 1.000 val_loss: 1.325 val_acc: 0.692\n",
      "Epoch 21 loss: 0.001 acc: 1.000 val_loss: 1.357 val_acc: 0.698\n",
      "Epoch 22 loss: 0.001 acc: 1.000 val_loss: 1.338 val_acc: 0.697\n",
      "Epoch 23 loss: 0.001 acc: 1.000 val_loss: 1.368 val_acc: 0.689\n",
      "Epoch 24 loss: 0.002 acc: 1.000 val_loss: 1.418 val_acc: 0.674\n",
      "Epoch 25 loss: 0.001 acc: 1.000 val_loss: 1.388 val_acc: 0.696\n",
      "Epoch 26 loss: 0.001 acc: 1.000 val_loss: 1.426 val_acc: 0.688\n",
      "Epoch 27 loss: 0.001 acc: 1.000 val_loss: 1.456 val_acc: 0.695\n",
      "Epoch 28 loss: 0.001 acc: 1.000 val_loss: 1.464 val_acc: 0.693\n",
      "Epoch 29 loss: 0.001 acc: 1.000 val_loss: 1.461 val_acc: 0.694\n",
      "Epoch 30 loss: 0.001 acc: 1.000 val_loss: 1.517 val_acc: 0.699\n",
      "Epoch 31 loss: 0.001 acc: 1.000 val_loss: 1.507 val_acc: 0.691\n",
      "Epoch 32 loss: 0.000 acc: 1.000 val_loss: 1.529 val_acc: 0.694\n",
      "Epoch 33 loss: 0.002 acc: 1.000 val_loss: 1.538 val_acc: 0.689\n",
      "Epoch 34 loss: 0.040 acc: 0.988 val_loss: 1.590 val_acc: 0.679\n",
      "Epoch 35 loss: 0.344 acc: 0.882 val_loss: 1.151 val_acc: 0.652\n",
      "Epoch 36 loss: 0.231 acc: 0.918 val_loss: 0.875 val_acc: 0.718\n",
      "Epoch 37 loss: 0.114 acc: 0.958 val_loss: 0.928 val_acc: 0.708\n",
      "Epoch 38 loss: 0.043 acc: 0.991 val_loss: 0.973 val_acc: 0.701\n",
      "Epoch 39 loss: 0.019 acc: 1.000 val_loss: 1.066 val_acc: 0.703\n",
      "Epoch 40 loss: 0.018 acc: 0.997 val_loss: 1.179 val_acc: 0.699\n",
      "Epoch 41 loss: 0.008 acc: 1.000 val_loss: 1.142 val_acc: 0.708\n",
      "Epoch 42 loss: 0.005 acc: 1.000 val_loss: 1.202 val_acc: 0.716\n",
      "Epoch 43 loss: 0.005 acc: 0.999 val_loss: 1.226 val_acc: 0.710\n",
      "Epoch 44 loss: 0.003 acc: 1.000 val_loss: 1.225 val_acc: 0.710\n",
      "Epoch 45 loss: 0.002 acc: 1.000 val_loss: 1.296 val_acc: 0.711\n",
      "Epoch 46 loss: 0.002 acc: 1.000 val_loss: 1.314 val_acc: 0.711\n",
      "Epoch 47 loss: 0.002 acc: 1.000 val_loss: 1.289 val_acc: 0.711\n",
      "Epoch 48 loss: 0.001 acc: 1.000 val_loss: 1.433 val_acc: 0.708\n",
      "Epoch 49 loss: 0.001 acc: 1.000 val_loss: 1.353 val_acc: 0.711\n",
      "Epoch 50 loss: 0.001 acc: 1.000 val_loss: 1.326 val_acc: 0.706\n",
      "Epoch 51 loss: 0.001 acc: 1.000 val_loss: 1.389 val_acc: 0.707\n",
      "Epoch 52 loss: 0.001 acc: 1.000 val_loss: 1.376 val_acc: 0.710\n",
      "Epoch 53 loss: 0.001 acc: 1.000 val_loss: 1.420 val_acc: 0.719\n",
      "Epoch 54 loss: 0.001 acc: 1.000 val_loss: 1.393 val_acc: 0.711\n",
      "Epoch 55 loss: 0.001 acc: 1.000 val_loss: 1.401 val_acc: 0.710\n",
      "Epoch 56 loss: 0.001 acc: 1.000 val_loss: 1.439 val_acc: 0.715\n",
      "Epoch 57 loss: 0.001 acc: 1.000 val_loss: 1.451 val_acc: 0.703\n",
      "Epoch 58 loss: 0.001 acc: 1.000 val_loss: 1.426 val_acc: 0.709\n",
      "Epoch 59 loss: 0.001 acc: 1.000 val_loss: 1.510 val_acc: 0.705\n",
      "Epoch 60 loss: 0.000 acc: 1.000 val_loss: 1.434 val_acc: 0.714\n",
      "Epoch 61 loss: 0.001 acc: 1.000 val_loss: 1.503 val_acc: 0.710\n",
      "Epoch 62 loss: 0.001 acc: 1.000 val_loss: 1.551 val_acc: 0.710\n",
      "Epoch 63 loss: 0.001 acc: 1.000 val_loss: 1.493 val_acc: 0.711\n",
      "Epoch 64 loss: 0.000 acc: 1.000 val_loss: 1.583 val_acc: 0.707\n",
      "Epoch 65 loss: 0.001 acc: 1.000 val_loss: 1.610 val_acc: 0.706\n",
      "Epoch 66 loss: 0.000 acc: 1.000 val_loss: 1.532 val_acc: 0.715\n",
      "Epoch 67 loss: 0.000 acc: 1.000 val_loss: 1.551 val_acc: 0.704\n",
      "Epoch 68 loss: 0.000 acc: 1.000 val_loss: 1.521 val_acc: 0.709\n",
      "Epoch 69 loss: 0.000 acc: 1.000 val_loss: 1.556 val_acc: 0.705\n",
      "Epoch 70 loss: 0.000 acc: 1.000 val_loss: 1.528 val_acc: 0.706\n",
      "Epoch 71 loss: 0.000 acc: 1.000 val_loss: 1.524 val_acc: 0.708\n",
      "Epoch 72 loss: 0.000 acc: 1.000 val_loss: 1.498 val_acc: 0.720\n",
      "Epoch 73 loss: 0.000 acc: 1.000 val_loss: 1.499 val_acc: 0.721\n",
      "Epoch 74 loss: 0.000 acc: 1.000 val_loss: 1.596 val_acc: 0.712\n",
      "Epoch 75 loss: 0.000 acc: 1.000 val_loss: 1.589 val_acc: 0.714\n",
      "Epoch 76 loss: 0.000 acc: 1.000 val_loss: 1.547 val_acc: 0.717\n",
      "Epoch 77 loss: 0.000 acc: 1.000 val_loss: 1.612 val_acc: 0.710\n",
      "Epoch 78 loss: 0.000 acc: 1.000 val_loss: 1.621 val_acc: 0.702\n",
      "Epoch 79 loss: 0.000 acc: 1.000 val_loss: 1.938 val_acc: 0.713\n",
      "Epoch 80 loss: 0.000 acc: 1.000 val_loss: 1.584 val_acc: 0.708\n",
      "Epoch 81 loss: 0.000 acc: 1.000 val_loss: 1.612 val_acc: 0.703\n",
      "Epoch 82 loss: 0.000 acc: 1.000 val_loss: 1.643 val_acc: 0.701\n",
      "Epoch 83 loss: 0.000 acc: 1.000 val_loss: 1.610 val_acc: 0.709\n",
      "Epoch 84 loss: 0.000 acc: 1.000 val_loss: 1.631 val_acc: 0.713\n",
      "Epoch 85 loss: 0.000 acc: 1.000 val_loss: 1.662 val_acc: 0.715\n",
      "Epoch 86 loss: 0.000 acc: 1.000 val_loss: 1.613 val_acc: 0.719\n",
      "Epoch 87 loss: 0.000 acc: 1.000 val_loss: 1.652 val_acc: 0.714\n",
      "Epoch 88 loss: 0.000 acc: 1.000 val_loss: 1.691 val_acc: 0.705\n",
      "Epoch 89 loss: 0.000 acc: 1.000 val_loss: 1.605 val_acc: 0.721\n",
      "Epoch 90 loss: 0.000 acc: 1.000 val_loss: 1.663 val_acc: 0.701\n",
      "Epoch 91 loss: 0.000 acc: 1.000 val_loss: 1.648 val_acc: 0.712\n",
      "Epoch 92 loss: 0.000 acc: 1.000 val_loss: 1.676 val_acc: 0.716\n",
      "Epoch 93 loss: 0.000 acc: 1.000 val_loss: 1.709 val_acc: 0.703\n",
      "Epoch 94 loss: 0.000 acc: 1.000 val_loss: 1.665 val_acc: 0.714\n",
      "Epoch 95 loss: 0.000 acc: 1.000 val_loss: 1.687 val_acc: 0.706\n",
      "Epoch 96 loss: 0.000 acc: 1.000 val_loss: 1.694 val_acc: 0.708\n",
      "Epoch 97 loss: 0.000 acc: 1.000 val_loss: 1.746 val_acc: 0.705\n",
      "Epoch 98 loss: 0.000 acc: 1.000 val_loss: 1.697 val_acc: 0.708\n",
      "Epoch 99 loss: 0.000 acc: 1.000 val_loss: 1.727 val_acc: 0.699\n",
      "Epoch 100 loss: 0.000 acc: 1.000 val_loss: 1.720 val_acc: 0.716\n"
     ]
    }
   ],
   "source": [
    "history_10, net_10 = define_and_train(NN_convolutional6, 100, training_set, test_set, batch_size=32, weight_decay= 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-option",
   "metadata": {},
   "source": [
    "Batch normalisation does not seem to help again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-token",
   "metadata": {},
   "source": [
    "# Option 11: 5 layer CNN network with dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "suitable-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional7( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional7, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            #layer 1\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 2\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 3\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 4\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 5\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #Output process \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "intimate-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.694 acc: 0.492 val_loss: 0.695 val_acc: 0.472\n",
      "Epoch 2 loss: 0.694 acc: 0.492 val_loss: 0.694 val_acc: 0.472\n",
      "Epoch 3 loss: 0.693 acc: 0.494 val_loss: 0.694 val_acc: 0.474\n",
      "Epoch 4 loss: 0.693 acc: 0.477 val_loss: 0.693 val_acc: 0.490\n",
      "Epoch 5 loss: 0.693 acc: 0.523 val_loss: 0.692 val_acc: 0.532\n",
      "Epoch 6 loss: 0.682 acc: 0.535 val_loss: 0.651 val_acc: 0.602\n",
      "Epoch 7 loss: 0.695 acc: 0.539 val_loss: 0.695 val_acc: 0.472\n",
      "Epoch 8 loss: 0.694 acc: 0.492 val_loss: 0.694 val_acc: 0.472\n",
      "Epoch 9 loss: 0.694 acc: 0.492 val_loss: 0.694 val_acc: 0.472\n",
      "Epoch 10 loss: 0.693 acc: 0.492 val_loss: 0.694 val_acc: 0.472\n",
      "Epoch 11 loss: 0.693 acc: 0.492 val_loss: 0.694 val_acc: 0.472\n",
      "Epoch 12 loss: 0.693 acc: 0.492 val_loss: 0.693 val_acc: 0.472\n",
      "Epoch 13 loss: 0.693 acc: 0.490 val_loss: 0.693 val_acc: 0.472\n",
      "Epoch 14 loss: 0.693 acc: 0.514 val_loss: 0.693 val_acc: 0.524\n",
      "Epoch 15 loss: 0.693 acc: 0.486 val_loss: 0.693 val_acc: 0.519\n",
      "Epoch 16 loss: 0.693 acc: 0.532 val_loss: 0.693 val_acc: 0.515\n",
      "Epoch 17 loss: 0.692 acc: 0.506 val_loss: 0.691 val_acc: 0.503\n",
      "Epoch 18 loss: 0.691 acc: 0.511 val_loss: 0.690 val_acc: 0.523\n",
      "Epoch 19 loss: 0.686 acc: 0.525 val_loss: 0.687 val_acc: 0.492\n",
      "Epoch 20 loss: 0.690 acc: 0.564 val_loss: 0.686 val_acc: 0.597\n",
      "Epoch 21 loss: 0.679 acc: 0.575 val_loss: 0.665 val_acc: 0.590\n",
      "Epoch 22 loss: 0.646 acc: 0.665 val_loss: 0.632 val_acc: 0.699\n",
      "Epoch 23 loss: 0.601 acc: 0.712 val_loss: 0.663 val_acc: 0.639\n",
      "Epoch 24 loss: 0.586 acc: 0.698 val_loss: 0.592 val_acc: 0.710\n",
      "Epoch 25 loss: 0.647 acc: 0.641 val_loss: 0.578 val_acc: 0.706\n",
      "Epoch 26 loss: 0.556 acc: 0.738 val_loss: 0.608 val_acc: 0.662\n",
      "Epoch 27 loss: 0.553 acc: 0.738 val_loss: 0.509 val_acc: 0.754\n",
      "Epoch 28 loss: 0.507 acc: 0.753 val_loss: 0.525 val_acc: 0.745\n",
      "Epoch 29 loss: 0.520 acc: 0.753 val_loss: 0.527 val_acc: 0.735\n",
      "Epoch 30 loss: 0.509 acc: 0.770 val_loss: 0.497 val_acc: 0.768\n",
      "Epoch 31 loss: 0.479 acc: 0.776 val_loss: 0.490 val_acc: 0.780\n",
      "Epoch 32 loss: 0.460 acc: 0.785 val_loss: 0.488 val_acc: 0.751\n",
      "Epoch 33 loss: 0.422 acc: 0.807 val_loss: 0.457 val_acc: 0.793\n",
      "Epoch 34 loss: 0.466 acc: 0.787 val_loss: 0.467 val_acc: 0.776\n",
      "Epoch 35 loss: 0.453 acc: 0.799 val_loss: 0.439 val_acc: 0.781\n",
      "Epoch 36 loss: 0.424 acc: 0.801 val_loss: 0.435 val_acc: 0.803\n",
      "Epoch 37 loss: 0.397 acc: 0.827 val_loss: 0.429 val_acc: 0.807\n",
      "Epoch 38 loss: 0.373 acc: 0.838 val_loss: 0.399 val_acc: 0.831\n",
      "Epoch 39 loss: 0.382 acc: 0.845 val_loss: 0.383 val_acc: 0.825\n",
      "Epoch 40 loss: 0.396 acc: 0.839 val_loss: 0.395 val_acc: 0.830\n",
      "Epoch 41 loss: 0.322 acc: 0.858 val_loss: 0.408 val_acc: 0.813\n",
      "Epoch 42 loss: 0.312 acc: 0.864 val_loss: 0.393 val_acc: 0.837\n",
      "Epoch 43 loss: 0.349 acc: 0.860 val_loss: 0.339 val_acc: 0.855\n",
      "Epoch 44 loss: 0.318 acc: 0.859 val_loss: 0.330 val_acc: 0.862\n",
      "Epoch 45 loss: 0.283 acc: 0.878 val_loss: 0.366 val_acc: 0.841\n",
      "Epoch 46 loss: 0.297 acc: 0.867 val_loss: 0.380 val_acc: 0.847\n",
      "Epoch 47 loss: 0.307 acc: 0.871 val_loss: 0.327 val_acc: 0.858\n",
      "Epoch 48 loss: 0.275 acc: 0.889 val_loss: 0.312 val_acc: 0.873\n",
      "Epoch 49 loss: 0.243 acc: 0.902 val_loss: 0.391 val_acc: 0.841\n",
      "Epoch 50 loss: 0.257 acc: 0.889 val_loss: 0.300 val_acc: 0.872\n",
      "Epoch 51 loss: 0.268 acc: 0.896 val_loss: 0.327 val_acc: 0.870\n",
      "Epoch 52 loss: 0.261 acc: 0.887 val_loss: 0.274 val_acc: 0.896\n",
      "Epoch 53 loss: 0.312 acc: 0.879 val_loss: 0.326 val_acc: 0.861\n",
      "Epoch 54 loss: 0.221 acc: 0.907 val_loss: 0.307 val_acc: 0.878\n",
      "Epoch 55 loss: 0.231 acc: 0.914 val_loss: 0.278 val_acc: 0.885\n",
      "Epoch 56 loss: 0.229 acc: 0.909 val_loss: 0.267 val_acc: 0.890\n",
      "Epoch 57 loss: 0.224 acc: 0.899 val_loss: 0.302 val_acc: 0.875\n",
      "Epoch 58 loss: 0.249 acc: 0.901 val_loss: 0.340 val_acc: 0.861\n",
      "Epoch 59 loss: 0.230 acc: 0.909 val_loss: 0.264 val_acc: 0.896\n",
      "Epoch 60 loss: 0.203 acc: 0.919 val_loss: 0.249 val_acc: 0.896\n",
      "Epoch 61 loss: 0.215 acc: 0.916 val_loss: 0.322 val_acc: 0.876\n",
      "Epoch 62 loss: 0.236 acc: 0.908 val_loss: 0.269 val_acc: 0.894\n",
      "Epoch 63 loss: 0.209 acc: 0.923 val_loss: 0.269 val_acc: 0.891\n",
      "Epoch 64 loss: 0.196 acc: 0.923 val_loss: 0.243 val_acc: 0.903\n",
      "Epoch 65 loss: 0.182 acc: 0.934 val_loss: 0.280 val_acc: 0.896\n",
      "Epoch 66 loss: 0.184 acc: 0.927 val_loss: 0.230 val_acc: 0.915\n",
      "Epoch 67 loss: 0.204 acc: 0.928 val_loss: 0.356 val_acc: 0.861\n",
      "Epoch 68 loss: 0.189 acc: 0.924 val_loss: 0.277 val_acc: 0.889\n",
      "Epoch 69 loss: 0.201 acc: 0.928 val_loss: 0.238 val_acc: 0.897\n",
      "Epoch 70 loss: 0.154 acc: 0.939 val_loss: 0.227 val_acc: 0.906\n",
      "Epoch 71 loss: 0.166 acc: 0.938 val_loss: 0.265 val_acc: 0.901\n",
      "Epoch 72 loss: 0.172 acc: 0.936 val_loss: 0.243 val_acc: 0.904\n",
      "Epoch 73 loss: 0.161 acc: 0.938 val_loss: 0.238 val_acc: 0.908\n",
      "Epoch 74 loss: 0.172 acc: 0.943 val_loss: 0.239 val_acc: 0.908\n",
      "Epoch 75 loss: 0.164 acc: 0.937 val_loss: 0.231 val_acc: 0.913\n",
      "Epoch 76 loss: 0.157 acc: 0.937 val_loss: 0.211 val_acc: 0.920\n",
      "Epoch 77 loss: 0.146 acc: 0.940 val_loss: 0.217 val_acc: 0.912\n",
      "Epoch 78 loss: 0.160 acc: 0.939 val_loss: 0.241 val_acc: 0.908\n",
      "Epoch 79 loss: 0.165 acc: 0.932 val_loss: 0.225 val_acc: 0.911\n",
      "Epoch 80 loss: 0.136 acc: 0.950 val_loss: 0.223 val_acc: 0.915\n",
      "Epoch 81 loss: 0.129 acc: 0.950 val_loss: 0.263 val_acc: 0.905\n",
      "Epoch 82 loss: 0.157 acc: 0.934 val_loss: 0.235 val_acc: 0.910\n",
      "Epoch 83 loss: 0.151 acc: 0.937 val_loss: 0.272 val_acc: 0.906\n",
      "Epoch 84 loss: 0.162 acc: 0.938 val_loss: 0.240 val_acc: 0.911\n",
      "Epoch 85 loss: 0.153 acc: 0.933 val_loss: 0.218 val_acc: 0.923\n",
      "Epoch 86 loss: 0.141 acc: 0.945 val_loss: 0.209 val_acc: 0.919\n",
      "Epoch 87 loss: 0.139 acc: 0.950 val_loss: 0.382 val_acc: 0.885\n",
      "Epoch 88 loss: 0.145 acc: 0.953 val_loss: 0.200 val_acc: 0.922\n",
      "Epoch 89 loss: 0.136 acc: 0.960 val_loss: 0.228 val_acc: 0.914\n",
      "Epoch 90 loss: 0.142 acc: 0.948 val_loss: 0.224 val_acc: 0.915\n",
      "Epoch 91 loss: 0.138 acc: 0.947 val_loss: 0.203 val_acc: 0.931\n",
      "Epoch 92 loss: 0.143 acc: 0.947 val_loss: 0.221 val_acc: 0.913\n",
      "Epoch 93 loss: 0.117 acc: 0.956 val_loss: 0.222 val_acc: 0.926\n",
      "Epoch 94 loss: 0.131 acc: 0.952 val_loss: 0.190 val_acc: 0.932\n",
      "Epoch 95 loss: 0.110 acc: 0.963 val_loss: 0.226 val_acc: 0.921\n",
      "Epoch 96 loss: 0.120 acc: 0.952 val_loss: 0.187 val_acc: 0.924\n",
      "Epoch 97 loss: 0.123 acc: 0.954 val_loss: 0.239 val_acc: 0.917\n",
      "Epoch 98 loss: 0.117 acc: 0.957 val_loss: 0.233 val_acc: 0.918\n",
      "Epoch 99 loss: 0.135 acc: 0.949 val_loss: 0.229 val_acc: 0.926\n",
      "Epoch 100 loss: 0.140 acc: 0.953 val_loss: 0.186 val_acc: 0.920\n"
     ]
    }
   ],
   "source": [
    "history_11, net_11 = define_and_train(NN_convolutional7, 100, training_set, test_set, batch_size=32, weight_decay= 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-sacramento",
   "metadata": {},
   "source": [
    "Dropout results in better peformance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-marine",
   "metadata": {},
   "source": [
    "# Option 12: 10 layer network with dropout and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "outstanding-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_convolutional8( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN_convolutional8, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(1,12,3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            #layer 1\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 2\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 3\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 4\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #layer 5\n",
    "            nn.Conv2d(12,12,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            #layer 6\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            #layer 7\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            #layer 8\n",
    "            nn.Conv2d(12,12,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            #layer 9\n",
    "            nn.Conv2d(12,2,3,padding=1), # idea: hopefully these will be connected to left edge, connected to right edge\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            #Output process \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,12), # the 128 here is the number of elements in the tensor computed so far\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12,1),\n",
    "            nn.Sigmoid())  # we are predicting only two classes, so we can use one sigmoid neuron as output\n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "empirical-cement",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.695 acc: 0.508 val_loss: 0.691 val_acc: 0.528\n",
      "Epoch 2 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 3 loss: 0.694 acc: 0.507 val_loss: 0.693 val_acc: 0.498\n",
      "Epoch 4 loss: 0.694 acc: 0.506 val_loss: 0.693 val_acc: 0.526\n",
      "Epoch 5 loss: 0.693 acc: 0.505 val_loss: 0.692 val_acc: 0.515\n",
      "Epoch 6 loss: 0.692 acc: 0.504 val_loss: 0.685 val_acc: 0.497\n",
      "Epoch 7 loss: 0.685 acc: 0.570 val_loss: 0.685 val_acc: 0.592\n",
      "Epoch 8 loss: 0.693 acc: 0.510 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 9 loss: 0.694 acc: 0.508 val_loss: 0.691 val_acc: 0.528\n",
      "Epoch 10 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 11 loss: 0.693 acc: 0.508 val_loss: 0.693 val_acc: 0.527\n",
      "Epoch 12 loss: 0.694 acc: 0.506 val_loss: 0.692 val_acc: 0.526\n",
      "Epoch 13 loss: 0.693 acc: 0.505 val_loss: 0.692 val_acc: 0.521\n",
      "Epoch 14 loss: 0.693 acc: 0.509 val_loss: 0.692 val_acc: 0.525\n",
      "Epoch 15 loss: 0.693 acc: 0.506 val_loss: 0.691 val_acc: 0.526\n",
      "Epoch 16 loss: 0.693 acc: 0.506 val_loss: 0.692 val_acc: 0.525\n",
      "Epoch 17 loss: 0.693 acc: 0.508 val_loss: 0.691 val_acc: 0.529\n",
      "Epoch 18 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 19 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.529\n",
      "Epoch 20 loss: 0.693 acc: 0.514 val_loss: 0.693 val_acc: 0.530\n",
      "Epoch 21 loss: 0.693 acc: 0.505 val_loss: 0.693 val_acc: 0.524\n",
      "Epoch 22 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 23 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.530\n",
      "Epoch 24 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.538\n",
      "Epoch 25 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 26 loss: 0.693 acc: 0.514 val_loss: 0.693 val_acc: 0.527\n",
      "Epoch 27 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.522\n",
      "Epoch 28 loss: 0.694 acc: 0.505 val_loss: 0.692 val_acc: 0.529\n",
      "Epoch 29 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 30 loss: 0.694 acc: 0.505 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 31 loss: 0.693 acc: 0.510 val_loss: 0.692 val_acc: 0.522\n",
      "Epoch 32 loss: 0.694 acc: 0.478 val_loss: 0.692 val_acc: 0.535\n",
      "Epoch 33 loss: 0.693 acc: 0.512 val_loss: 0.693 val_acc: 0.526\n",
      "Epoch 34 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 35 loss: 0.694 acc: 0.508 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 36 loss: 0.693 acc: 0.509 val_loss: 0.693 val_acc: 0.525\n",
      "Epoch 37 loss: 0.693 acc: 0.511 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 38 loss: 0.694 acc: 0.489 val_loss: 0.693 val_acc: 0.530\n",
      "Epoch 39 loss: 0.693 acc: 0.495 val_loss: 0.693 val_acc: 0.511\n",
      "Epoch 40 loss: 0.693 acc: 0.509 val_loss: 0.693 val_acc: 0.530\n",
      "Epoch 41 loss: 0.693 acc: 0.490 val_loss: 0.693 val_acc: 0.511\n",
      "Epoch 42 loss: 0.692 acc: 0.532 val_loss: 0.693 val_acc: 0.510\n",
      "Epoch 43 loss: 0.694 acc: 0.488 val_loss: 0.693 val_acc: 0.520\n",
      "Epoch 44 loss: 0.694 acc: 0.476 val_loss: 0.692 val_acc: 0.525\n",
      "Epoch 45 loss: 0.694 acc: 0.503 val_loss: 0.691 val_acc: 0.528\n",
      "Epoch 46 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 47 loss: 0.693 acc: 0.505 val_loss: 0.692 val_acc: 0.546\n",
      "Epoch 48 loss: 0.694 acc: 0.494 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 49 loss: 0.693 acc: 0.504 val_loss: 0.693 val_acc: 0.520\n",
      "Epoch 50 loss: 0.693 acc: 0.505 val_loss: 0.692 val_acc: 0.518\n",
      "Epoch 51 loss: 0.695 acc: 0.507 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 52 loss: 0.693 acc: 0.510 val_loss: 0.692 val_acc: 0.531\n",
      "Epoch 53 loss: 0.693 acc: 0.508 val_loss: 0.693 val_acc: 0.512\n",
      "Epoch 54 loss: 0.693 acc: 0.516 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 55 loss: 0.693 acc: 0.502 val_loss: 0.693 val_acc: 0.524\n",
      "Epoch 56 loss: 0.693 acc: 0.508 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 57 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.529\n",
      "Epoch 58 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 59 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 60 loss: 0.694 acc: 0.507 val_loss: 0.692 val_acc: 0.529\n",
      "Epoch 61 loss: 0.693 acc: 0.487 val_loss: 0.693 val_acc: 0.498\n",
      "Epoch 62 loss: 0.693 acc: 0.501 val_loss: 0.692 val_acc: 0.521\n",
      "Epoch 63 loss: 0.694 acc: 0.496 val_loss: 0.693 val_acc: 0.508\n",
      "Epoch 64 loss: 0.693 acc: 0.514 val_loss: 0.693 val_acc: 0.527\n",
      "Epoch 65 loss: 0.694 acc: 0.510 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 66 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 67 loss: 0.693 acc: 0.513 val_loss: 0.692 val_acc: 0.529\n",
      "Epoch 68 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 69 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 70 loss: 0.693 acc: 0.509 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 71 loss: 0.693 acc: 0.509 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 72 loss: 0.694 acc: 0.509 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 73 loss: 0.693 acc: 0.507 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 74 loss: 0.694 acc: 0.482 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 75 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 76 loss: 0.694 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 77 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.522\n",
      "Epoch 78 loss: 0.693 acc: 0.506 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 79 loss: 0.693 acc: 0.512 val_loss: 0.692 val_acc: 0.525\n",
      "Epoch 80 loss: 0.693 acc: 0.509 val_loss: 0.693 val_acc: 0.520\n",
      "Epoch 81 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 82 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 83 loss: 0.693 acc: 0.514 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 84 loss: 0.693 acc: 0.505 val_loss: 0.693 val_acc: 0.527\n",
      "Epoch 85 loss: 0.693 acc: 0.501 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 86 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 87 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 88 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 89 loss: 0.694 acc: 0.507 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 90 loss: 0.693 acc: 0.507 val_loss: 0.692 val_acc: 0.523\n",
      "Epoch 91 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 92 loss: 0.693 acc: 0.509 val_loss: 0.692 val_acc: 0.527\n",
      "Epoch 93 loss: 0.693 acc: 0.510 val_loss: 0.693 val_acc: 0.525\n",
      "Epoch 94 loss: 0.693 acc: 0.510 val_loss: 0.692 val_acc: 0.531\n",
      "Epoch 95 loss: 0.693 acc: 0.506 val_loss: 0.692 val_acc: 0.526\n",
      "Epoch 96 loss: 0.693 acc: 0.503 val_loss: 0.692 val_acc: 0.528\n",
      "Epoch 97 loss: 0.693 acc: 0.508 val_loss: 0.692 val_acc: 0.530\n",
      "Epoch 98 loss: 0.693 acc: 0.509 val_loss: 0.693 val_acc: 0.528\n",
      "Epoch 99 loss: 0.693 acc: 0.501 val_loss: 0.693 val_acc: 0.523\n",
      "Epoch 100 loss: 0.693 acc: 0.510 val_loss: 0.692 val_acc: 0.533\n"
     ]
    }
   ],
   "source": [
    "history_12, net_12 = define_and_train(NN_convolutional8, 100, training_set, test_set, batch_size=32, weight_decay= 1e-5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}